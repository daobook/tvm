
<!DOCTYPE html>

<html lang="zh_CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>2D Convolution Optimization &#8212; TVM  文档</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../../../_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../../../" id="documentation_options" src="../../../../../_static/documentation_options.js"></script>
    <script src="../../../../../_static/jquery.js"></script>
    <script src="../../../../../_static/underscore.js"></script>
    <script src="../../../../../_static/doctools.js"></script>
    <script src="../../../../../_static/clipboard.min.js"></script>
    <script src="../../../../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../../../_static/togglebutton.js"></script>
    <script src="../../../../../_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script src="../../../../../_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <link rel="shortcut icon" href="../../../../../_static/tvm-logo-square.png"/>
    <link rel="index" title="索引" href="../../../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../../../search.html" />
    <link rel="next" title="Matrix Multiply Blocking" href="matrix_multiply_opt.html" />
    <link rel="prev" title="Optimize Tensor Operators" href="index.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="zh_CN">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../../../_static/tvm-logo-small.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">TVM  文档</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../README.html">
   TVM 文档
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../../../start.html">
   快速上手
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../../../install/index.html">
     安装 TVM
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../../../install/from_source.html">
       从源码安装
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
      <label for="toctree-checkbox-3">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../install/nnpack.html">
         NNPACK Contrib Installation
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../install/docker.html">
       Docker 镜像
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../install/tlcpack.html">
       TLCPack
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../../../contribute/index.html">
     贡献者指南
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
    <label for="toctree-checkbox-4">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../contribute/community.html">
       TVM 社区指南
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../contribute/pull_request.html">
       提交 Pull Request
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../contribute/code_review.html">
       Code Reviews
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../contribute/committer_guide.html">
       Committer Guide
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../contribute/document.html">
       Documentation
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../contribute/code_guide.html">
       Code Guide and Tips
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../contribute/git_howto.html">
       Git Usage Tips
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../contribute/ci.html">
       Using TVM’s CI
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../contribute/release_process.html">
       Release Process
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../contribute/error_handling.html">
       Error Handling Guide
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../../../study/index.html">
   学习笔记
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../../study/quantize.html">
     量化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../../study/start.html">
     TVM 入门指南
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../../study/test.html">
     测试 TVM
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../../../user-guide.html">
   用户手册
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../../../tutorial/index.html">
     用户指南
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../tutorial/introduction.html">
       TVM 和模型优化的概述
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../tutorial/tvmc_command_line_driver.html">
       用 TVMC 编译和优化模型
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../tutorial/tvmc_python.html">
       开始使用 TVMC Python：TVM 的高级 API
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../tutorial/autotvm_relay_x86.html">
       用 Python 接口编译和优化模型
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../tutorial/tensor_expr_get_started.html">
       使用张量表达式处理算子
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../tutorial/autotvm_matmul_x86.html">
       用调度模板和 AutoTVM 优化算子
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../tutorial/auto_scheduler_matmul_x86.html">
       使用自动调度优化运算
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../tutorial/tensor_ir_blitz_course.html">
       TensorIR 的突击课程
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../tutorial/cross_compilation_and_rpc.html">
       交叉编译和RPC
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../tutorial/relay_quick_start.html">
       编译深度学习模型的快速入门教程
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../tutorial/intro_topi.html">
       TOPI 简介
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../../../how_to/index.html">
     How To 指南
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
    <label for="toctree-checkbox-8">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../../../how_to/compile_models/index.html">
       编译深度学习模型
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
      <label for="toctree-checkbox-9">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/compile_models/from_pytorch.html">
         编译 PyTorch 模型
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/compile_models/from_tensorflow.html">
         Compile Tensorflow Models
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/compile_models/from_mxnet.html">
         Compile MXNet Models
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/compile_models/from_onnx.html">
         Compile ONNX Models
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/compile_models/from_keras.html">
         Compile Keras Models
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/compile_models/from_tflite.html">
         Compile TFLite Models
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/compile_models/from_coreml.html">
         Compile CoreML Models
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/compile_models/from_darknet.html">
         Compile YOLO-V2 and YOLO-V3 in DarkNet Models
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/compile_models/from_caffe2.html">
         Compile Caffe2 Models
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/compile_models/from_paddle.html">
         Compile PaddlePaddle Models
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../../../how_to/deploy/index.html">
       部署模型，集成 TVM
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
      <label for="toctree-checkbox-10">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/deploy/cpp_deploy.html">
         Deploy TVM Module using C++ API
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/deploy/android.html">
         Deploy to Android
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/deploy/integrate.html">
         Integrate TVM into Your Project
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/deploy/hls.html">
         HLS Backend Example
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/deploy/arm_compute_lib.html">
         Relay Arm
         <sup>
          ®
         </sup>
         Compute Library Integration
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/deploy/tensorrt.html">
         Relay TensorRT Integration
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/deploy/vitis_ai.html">
         Vitis AI Integration
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/deploy/bnns.html">
         Relay BNNS Integration
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/deploy_models/index.html">
         部署深度学习模型
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../../../how_to/work_with_relay/index.html">
       Work With Relay
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
      <label for="toctree-checkbox-11">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul class="simple">
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../../../how_to/work_with_schedules/index.html">
       Work With Tensor Expression and Schedules
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
      <label for="toctree-checkbox-12">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul class="simple">
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../../../how_to/optimize_operators/index.html">
       Optimize Tensor Operators
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
      <label for="toctree-checkbox-13">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul class="simple">
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../../../how_to/tune_with_autotvm/index.html">
       Auto-Tune with Templates and AutoTVM
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
      <label for="toctree-checkbox-14">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/tune_with_autotvm/tune_conv2d_cuda.html">
         Tuning High Performance Convolution on NVIDIA GPUs
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/tune_with_autotvm/tune_relay_cuda.html">
         Auto-tuning a Convolutional Network for NVIDIA GPU
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/tune_with_autotvm/tune_relay_x86.html">
         Auto-tuning a Convolutional Network for x86 CPU
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/tune_with_autotvm/tune_relay_arm.html">
         Auto-tuning a Convolutional Network for ARM CPU
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/tune_with_autotvm/tune_relay_mobile_gpu.html">
         Auto-tuning a Convolutional Network for Mobile GPU
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../../../how_to/tune_with_autoscheduler/index.html">
       Use AutoScheduler for Template-Free Scheduling
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
      <label for="toctree-checkbox-15">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul class="simple">
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../../../how_to/work_with_microtvm/index.html">
       Work With microTVM
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
      <label for="toctree-checkbox-16">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul class="simple">
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../../../how_to/extend_tvm/index.html">
       Extend TVM
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
      <label for="toctree-checkbox-17">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul class="simple">
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../../../how_to/profile/index.html">
       Profile Models
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
      <label for="toctree-checkbox-18">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../../../how_to/profile/papi.html">
         Getting Started With PAPI
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../errors.html">
       处理 TVM 的错误
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../faq.html">
       常见问题
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../../../developer-guide.html">
   开发手册
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
  <label for="toctree-checkbox-19">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../../../dev/tutorial/index.html">
     开发者教程
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
    <label for="toctree-checkbox-20">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../dev/tutorial/codebase_walkthrough.html">
       TVM 代码库的实例演练
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../../../dev/how_to/how_to.html">
     开发者 How-To 指南
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
    <label for="toctree-checkbox-21">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../dev/how_to/relay_add_op.html">
       Adding an Operator to Relay
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../dev/how_to/relay_add_pass.html">
       Adding a Compiler Pass to Relay
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../dev/how_to/relay_bring_your_own_codegen.html">
       Bring Your Own Codegen To TVM
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../dev/how_to/pytest_target_parametrization.html">
       Python Target Parametrization
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../../arch/index.html">
   设计与架构
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
  <label for="toctree-checkbox-22">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../arch/runtime.html">
     TVM 运行时系统
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../arch/debugger.html">
     Debugger
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../arch/virtual_machine.html">
     Putting the VM in TVM: The Relay Virtual Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../arch/introduction_to_module_serialization.html">
     Introduction to Module Serialization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../arch/device_target_interactions.html">
     Device/Target Interactions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../arch/pass_infra.html">
     Pass Infrastructure
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../arch/device_target_interactions.html">
     Device/Target Interactions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../arch/inferbound.html">
     InferBound Pass
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../arch/hybrid_script.html">
     Hybrid Frontend Developer Guide
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../arch/relay_intro.html">
     Introduction to Relay IR
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../arch/relay_op_strategy.html">
     Relay Operator Strategy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../arch/convert_layout.html">
     Convert Layout Pass
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../arch/benchmark.html">
     Benchmark Performance Log Format
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../arch/frontend/tensorflow.html">
     TensorFlow Frontend
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../arch/security.html">
     Security Guide
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../arch/microtvm_design.html">
     microTVM Design Document
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../arch/microtvm_project_api.html">
     microTVM Project API
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../arch/model_library_format.html">
     Model Library Format
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../../../../../topic-guides.html">
   主题指南
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
  <label for="toctree-checkbox-23">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="../../../microtvm/index.html">
     microTVM：裸机上的 TVM
    </a>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="../../index.html">
     VTA：通用张量加速器
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/>
    <label for="toctree-checkbox-24">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3">
      <a class="reference internal" href="../../install.html">
       VTA 安装指南
      </a>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../dev/index.html">
       VTA 设计和开发指南
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/>
      <label for="toctree-checkbox-25">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../dev/config.html">
         VTA 配置
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../dev/hardware.html">
         VTA 硬件指南
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 current active has-children">
      <a class="reference internal" href="../index.html">
       VTA 教程
      </a>
      <input checked="" class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/>
      <label for="toctree-checkbox-26">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul class="current">
       <li class="toctree-l4">
        <a class="reference internal" href="../vta_get_started.html">
         VTA 入门
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../matrix_multiply.html">
         简单的矩阵乘法
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../frontend/index.html">
         编译深度学习模型
        </a>
       </li>
       <li class="toctree-l4 current active">
        <a class="reference internal" href="index.html">
         Optimize Tensor Operators
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../autotvm/index.html">
         Auto tuning
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../../../reference-guide.html">
   参考指南
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" type="checkbox"/>
  <label for="toctree-checkbox-27">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../../../reference/langref/index.html">
     语言参考
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" type="checkbox"/>
    <label for="toctree-checkbox-28">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/langref/relay_expr.html">
       Expressions in Relay
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/langref/relay_type.html">
       Relay’s Type System
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/langref/relay_adt.html">
       Algebraic Data Types in Relay
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/langref/relay_op.html">
       Relay Core Tensor Operators
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/langref/relay_pattern.html">
       Pattern Matching in Relay
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/langref/hybrid_script.html">
       Hybrid Frontend Language Reference
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../../../reference/api/python/index.html">
     Python API
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" type="checkbox"/>
    <label for="toctree-checkbox-29">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/runtime.html">
       tvm.runtime
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/ndarray.html">
       tvm.runtime.ndarray
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/error.html">
       tvm.error
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/ir.html">
       tvm.ir
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/target.html">
       tvm.target
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/tir.html">
       tvm.tir
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/te.html">
       tvm.te
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/driver.html">
       tvm.driver
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/relay/index.html">
       tvm.relay
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/relay/frontend.html">
       tvm.relay.frontend
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/relay/nn.html">
       tvm.relay.nn
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/relay/vision.html">
       tvm.relay.vision
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/relay/image.html">
       tvm.relay.image
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/relay/transform.html">
       tvm.relay.transform
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/relay/analysis.html">
       tvm.relay.analysis
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/relay/backend.html">
       tvm.relay.backend
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/relay/dataflow_pattern.html">
       tvm.relay.dataflow_pattern
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/relay/testing.html">
       tvm.relay.testing
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/autotvm.html">
       tvm.autotvm
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/auto_scheduler.html">
       tvm.auto_scheduler
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/rpc.html">
       tvm.rpc
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/micro.html">
       tvm.micro
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/contrib.html">
       tvm.contrib
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/graph_executor.html">
       tvm.contrib.graph_executor
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/topi.html">
       tvm.topi
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../reference/api/python/vta/index.html">
       vta
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../reference/api/links.html">
     其他 API
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../../reference/publications.html">
     出版物
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../../../refs/index.html">
   参考
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" type="checkbox"/>
  <label for="toctree-checkbox-30">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../../../../refs/_ffi/index.html">
     <code class="docutils literal notranslate">
      <span class="pre">
       _ffi
      </span>
     </code>
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" type="checkbox"/>
    <label for="toctree-checkbox-31">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../../refs/_ffi/base.html">
       <code class="docutils literal notranslate">
        <span class="pre">
         _ffi.base
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../../refs/_ffi/libinfo.html">
       <code class="docutils literal notranslate">
        <span class="pre">
         _ffi.libinfo
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../../refs/_ffi/object.html">
       <code class="docutils literal notranslate">
        <span class="pre">
         _ffi._ctypes.object
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../../refs/_ffi/registry.html">
       <code class="docutils literal notranslate">
        <span class="pre">
         _ffi.registry
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../../../refs/_ffi/runtime_ctypes.html">
       <code class="docutils literal notranslate">
        <span class="pre">
         _ffi.runtime_ctypes
        </span>
       </code>
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/xinetzone/torch-quantization/edit/main/docs/docs/topic/vta/tutorials/optimize/convolution_opt.rst"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../../../../_sources/docs/topic/vta/tutorials/optimize/convolution_opt.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> 导航
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rpc-setup">
   RPC Setup
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computation-declaration">
   Computation Declaration
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#scheduling-the-computation">
   Scheduling the Computation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#blocking-the-computation">
     Blocking the Computation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#virtual-threading">
     Virtual Threading
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lowering-copies-to-dma-transfers">
     Lowering Copies to DMA Transfers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lowering-computation-to-vta-compute-intrinsics">
     Lowering Computation to VTA Compute Intrinsics
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tvm-compilation-and-verification">
   TVM Compilation and Verification
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>2D Convolution Optimization</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> 导航 </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rpc-setup">
   RPC Setup
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computation-declaration">
   Computation Declaration
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#scheduling-the-computation">
   Scheduling the Computation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#blocking-the-computation">
     Blocking the Computation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#virtual-threading">
     Virtual Threading
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lowering-copies-to-dma-transfers">
     Lowering Copies to DMA Transfers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lowering-computation-to-vta-compute-intrinsics">
     Lowering Computation to VTA Compute Intrinsics
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tvm-compilation-and-verification">
   TVM Compilation and Verification
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">备注</p>
<p>Click <a class="reference external" href="https://tvm.apache.org/docs/topic/vta/tutorials/optimize/convolution_opt.html#sphx-glr-download-topic-vta-tutorials-optimize-convolution-opt-py" title="(在 tvm v0.9.dev0)"><span class="xref std std-ref">here</span></a>
to download the full example code</p>
</div>
<section class="sphx-glr-example-title" id="d-convolution-optimization">
<span id="sphx-glr-topic-vta-tutorials-optimize-convolution-opt-py"></span><h1>2D Convolution Optimization<a class="headerlink" href="#d-convolution-optimization" title="永久链接至标题">#</a></h1>
<p><strong>Author</strong>: <a class="reference external" href="https://homes.cs.washington.edu/~moreau/">Thierry Moreau</a></p>
<p>This tutorial provides an overview on how to use TVM to map a 2D convolution
workload efficiently on the VTA design.
We recommend covering the <a class="reference internal" href="matrix_multiply_opt.html#vta-mat-mult-opt"><span class="std std-ref">Matrix Multiply Blocking</span></a> tutorial first.</p>
<p>2D convolution is dominant in most computer vision deep neural networks.
In this tutorial, we will demonstrate TVM schedule optimizations to map
2D convolution operators in NCHW layout onto VTA.
We also introduce the notion of latency hiding, which allows us to
maximize VTA’s compute and memory resource utilization.</p>
<section id="rpc-setup">
<h2>RPC Setup<a class="headerlink" href="#rpc-setup" title="永久链接至标题">#</a></h2>
<p>We start by programming the Pynq’s FPGA and building its RPC runtime.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span><span class="p">,</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">tvm</span>
<span class="kn">import</span> <span class="nn">tvm.testing</span>
<span class="kn">from</span> <span class="nn">tvm</span> <span class="kn">import</span> <span class="n">te</span>
<span class="kn">import</span> <span class="nn">vta</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">tvm</span> <span class="kn">import</span> <span class="n">rpc</span>
<span class="kn">from</span> <span class="nn">tvm.contrib</span> <span class="kn">import</span> <span class="n">utils</span>
<span class="kn">from</span> <span class="nn">vta.testing</span> <span class="kn">import</span> <span class="n">simulator</span>

<span class="c1"># Load VTA parameters from the 3rdparty/vta-hw/config/vta_config.json file</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">vta</span><span class="o">.</span><span class="n">get_env</span><span class="p">()</span>

<span class="c1"># We read the Pynq RPC host IP address and port number from the OS environment</span>
<span class="n">host</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;VTA_RPC_HOST&quot;</span><span class="p">,</span> <span class="s2">&quot;192.168.2.99&quot;</span><span class="p">)</span>
<span class="n">port</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;VTA_RPC_PORT&quot;</span><span class="p">,</span> <span class="s2">&quot;9091&quot;</span><span class="p">))</span>

<span class="c1"># We configure both the bitstream and the runtime system on the Pynq</span>
<span class="c1"># to match the VTA configuration specified by the vta_config.json file.</span>
<span class="k">if</span> <span class="n">env</span><span class="o">.</span><span class="n">TARGET</span> <span class="o">==</span> <span class="s2">&quot;pynq&quot;</span><span class="p">:</span>

    <span class="c1"># Make sure that TVM was compiled with RPC=1</span>
    <span class="k">assert</span> <span class="n">tvm</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">enabled</span><span class="p">(</span><span class="s2">&quot;rpc&quot;</span><span class="p">)</span>
    <span class="n">remote</span> <span class="o">=</span> <span class="n">rpc</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="n">host</span><span class="p">,</span> <span class="n">port</span><span class="p">)</span>

    <span class="c1"># Reconfigure the JIT runtime</span>
    <span class="n">vta</span><span class="o">.</span><span class="n">reconfig_runtime</span><span class="p">(</span><span class="n">remote</span><span class="p">)</span>

    <span class="c1"># Program the FPGA with a pre-compiled VTA bitstream.</span>
    <span class="c1"># You can program the FPGA with your own custom bitstream</span>
    <span class="c1"># by passing the path to the bitstream file instead of None.</span>
    <span class="n">vta</span><span class="o">.</span><span class="n">program_fpga</span><span class="p">(</span><span class="n">remote</span><span class="p">,</span> <span class="n">bitstream</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># In simulation mode, host the RPC server locally.</span>
<span class="k">elif</span> <span class="n">env</span><span class="o">.</span><span class="n">TARGET</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;sim&quot;</span><span class="p">,</span> <span class="s2">&quot;tsim&quot;</span><span class="p">]:</span>
    <span class="n">remote</span> <span class="o">=</span> <span class="n">rpc</span><span class="o">.</span><span class="n">LocalSession</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="computation-declaration">
<h2>Computation Declaration<a class="headerlink" href="#computation-declaration" title="永久链接至标题">#</a></h2>
<p>As a first step, we need to describe our 2D convolution computation
in NCHW format.</p>
<p>We define the 2D convolution shape by the batch size,
spatial dimensions, input channels, output channels, kernel dimensions,
kernel dimensions, padding dimensions, and stride dimensions.</p>
<p>We pick the shape of the 9th convolutional layer of the ResNet-18
architecture as our convolution workload parameters.</p>
<p>We’ve added extra operators to the 2D convolution that apply
shifting and clipping to the output in order to mimic a fixed-point
convolution followed by a rectified linear activation.
We describe the TVM dataflow graph of the 2D convolution layer below:</p>
<img alt="https://raw.githubusercontent.com/uwsampl/web-data/main/vta/tutorial/conv2d_dataflow.png" class="align-center" src="https://raw.githubusercontent.com/uwsampl/web-data/main/vta/tutorial/conv2d_dataflow.png" />
<p>This computation is intentionally too large to fit onto VTA’s on-chip
buffers all at once. Therefore in the scheduling phase we’ll
rely on computation blocking strategies to break the computation down into
manageable chunks.</p>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p><em>Spatial padding</em></p>
<p>Note that we’ll need to import the TOPI library to apply spatial padding
on the input feature map tensor.
Spatial padding facilitates blocking in the context of 2D convolutions
due to the fact that the same (x, y) spatial location of the input
feature map of any given layer is read more than once if the convolution
kernel window size is greater than one.
On CPUs, and GPUs, one way to increase efficiency of memory accesses
when parallelizing work is spatial packing, which requires data re-layout.
VTA load DMA engine can insert padding automatically so that the original
input feature map does not have to be re-packed in memory.</p>
<p>We show the effect of VTA’s on the fly spatial padding when data is being
loaded from DRAM into VTA’s SRAM, following a 2D strided and padded memory
read.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/uwsampl/web-data/main/vta/tutorial/padding.png"><img alt="https://raw.githubusercontent.com/uwsampl/web-data/main/vta/tutorial/padding.png" class="align-center" src="https://raw.githubusercontent.com/uwsampl/web-data/main/vta/tutorial/padding.png" style="width: 480px;" /></a>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tvm</span> <span class="kn">import</span> <span class="n">topi</span>

<span class="c1"># 2D convolution layer dimensions taken from ResNet-18 architecture</span>
<span class="c1"># (9th convolutional layer)</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">height</span> <span class="o">=</span> <span class="mi">14</span>
<span class="n">width</span> <span class="o">=</span> <span class="mi">14</span>
<span class="n">in_channels</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">out_channels</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">kernel_h</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">kernel_w</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">pad_h</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">pad_w</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">stride_h</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">stride_w</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">assert</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="n">env</span><span class="o">.</span><span class="n">BATCH</span> <span class="o">==</span> <span class="mi">0</span>
<span class="k">assert</span> <span class="n">in_channels</span> <span class="o">%</span> <span class="n">env</span><span class="o">.</span><span class="n">BLOCK_IN</span> <span class="o">==</span> <span class="mi">0</span>
<span class="k">assert</span> <span class="n">out_channels</span> <span class="o">%</span> <span class="n">env</span><span class="o">.</span><span class="n">BLOCK_OUT</span> <span class="o">==</span> <span class="mi">0</span>

<span class="c1"># Input feature map: (N, IC, H, W, n, ic)</span>
<span class="n">data_shape</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">batch_size</span> <span class="o">//</span> <span class="n">env</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span>
    <span class="n">in_channels</span> <span class="o">//</span> <span class="n">env</span><span class="o">.</span><span class="n">BLOCK_IN</span><span class="p">,</span>
    <span class="n">height</span><span class="p">,</span>
    <span class="n">width</span><span class="p">,</span>
    <span class="n">env</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span>
    <span class="n">env</span><span class="o">.</span><span class="n">BLOCK_IN</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Kernel: (OC, IC, H, W, oc, ic)</span>
<span class="n">kernel_shape</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">out_channels</span> <span class="o">//</span> <span class="n">env</span><span class="o">.</span><span class="n">BLOCK_OUT</span><span class="p">,</span>
    <span class="n">in_channels</span> <span class="o">//</span> <span class="n">env</span><span class="o">.</span><span class="n">BLOCK_IN</span><span class="p">,</span>
    <span class="n">kernel_h</span><span class="p">,</span>
    <span class="n">kernel_w</span><span class="p">,</span>
    <span class="n">env</span><span class="o">.</span><span class="n">BLOCK_OUT</span><span class="p">,</span>
    <span class="n">env</span><span class="o">.</span><span class="n">BLOCK_IN</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Derive output feature map dimensions</span>
<span class="n">fout_height</span> <span class="o">=</span> <span class="p">(</span><span class="n">height</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pad_h</span> <span class="o">-</span> <span class="n">kernel_h</span><span class="p">)</span> <span class="o">//</span> <span class="n">stride_h</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">fout_width</span> <span class="o">=</span> <span class="p">(</span><span class="n">width</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pad_w</span> <span class="o">-</span> <span class="n">kernel_w</span><span class="p">)</span> <span class="o">//</span> <span class="n">stride_w</span> <span class="o">+</span> <span class="mi">1</span>
<span class="c1"># Output feature map: (N, OC, H, W, n, oc)</span>
<span class="n">output_shape</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">batch_size</span> <span class="o">//</span> <span class="n">env</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span>
    <span class="n">out_channels</span> <span class="o">//</span> <span class="n">env</span><span class="o">.</span><span class="n">BLOCK_OUT</span><span class="p">,</span>
    <span class="n">fout_height</span><span class="p">,</span>
    <span class="n">fout_width</span><span class="p">,</span>
    <span class="n">env</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span>
    <span class="n">env</span><span class="o">.</span><span class="n">BLOCK_OUT</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Convolution reduction axes</span>
<span class="n">dy</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">kernel_h</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dy&quot;</span><span class="p">)</span>
<span class="n">dx</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">kernel_w</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dx&quot;</span><span class="p">)</span>
<span class="n">ic</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">in_channels</span> <span class="o">//</span> <span class="n">env</span><span class="o">.</span><span class="n">BLOCK_IN</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;ic&quot;</span><span class="p">)</span>
<span class="n">ic_tns</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">BLOCK_IN</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;ic_tns&quot;</span><span class="p">)</span>

<span class="c1"># Input placeholder tensors</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">data_shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">inp_dtype</span><span class="p">)</span>
<span class="n">kernel</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">kernel_shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;kernel&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">wgt_dtype</span><span class="p">)</span>

<span class="c1"># Copy buffers:</span>
<span class="c1">#   Apply spatial padding to input feature map</span>
<span class="n">data_buf</span> <span class="o">=</span> <span class="n">topi</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">pad_h</span><span class="p">,</span> <span class="n">pad_w</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;data_buf&quot;</span><span class="p">)</span>
<span class="n">kernel_buf</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">kernel_shape</span><span class="p">,</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">i</span><span class="p">:</span> <span class="n">kernel</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">),</span> <span class="s2">&quot;kernel_buf&quot;</span><span class="p">)</span>

<span class="c1"># Declare 2D convolution</span>
<span class="n">res_conv</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span>
    <span class="n">output_shape</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">bo</span><span class="p">,</span> <span class="n">co</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">bi</span><span class="p">,</span> <span class="n">ci</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
        <span class="n">data_buf</span><span class="p">[</span><span class="n">bo</span><span class="p">,</span> <span class="n">ic</span><span class="p">,</span> <span class="n">i</span> <span class="o">*</span> <span class="n">stride_h</span> <span class="o">+</span> <span class="n">dy</span><span class="p">,</span> <span class="n">j</span> <span class="o">*</span> <span class="n">stride_w</span> <span class="o">+</span> <span class="n">dx</span><span class="p">,</span> <span class="n">bi</span><span class="p">,</span> <span class="n">ic_tns</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">acc_dtype</span><span class="p">)</span>
        <span class="o">*</span> <span class="n">kernel_buf</span><span class="p">[</span><span class="n">co</span><span class="p">,</span> <span class="n">ic</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">ci</span><span class="p">,</span> <span class="n">ic_tns</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">acc_dtype</span><span class="p">),</span>
        <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="n">ic</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">ic_tns</span><span class="p">],</span>
    <span class="p">),</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;res_conv&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Add shift stage for fix-point normalization</span>
<span class="n">res_shr</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">output_shape</span><span class="p">,</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">i</span><span class="p">:</span> <span class="n">res_conv</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="mi">8</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;res_shr&quot;</span><span class="p">)</span>

<span class="c1"># Apply clipping between (0, input max value)</span>
<span class="n">inp_max</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">INP_WIDTH</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">res_max</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">output_shape</span><span class="p">,</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">i</span><span class="p">:</span> <span class="n">tvm</span><span class="o">.</span><span class="n">te</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">res_shr</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="s2">&quot;res_max&quot;</span><span class="p">)</span>
<span class="n">res_min</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">output_shape</span><span class="p">,</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">i</span><span class="p">:</span> <span class="n">tvm</span><span class="o">.</span><span class="n">te</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">res_max</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">),</span> <span class="n">inp_max</span><span class="p">),</span> <span class="s2">&quot;res_min&quot;</span><span class="p">)</span>

<span class="c1"># Result Tensor</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">output_shape</span><span class="p">,</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">i</span><span class="p">:</span> <span class="n">res_min</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">inp_dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;res&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="scheduling-the-computation">
<h2>Scheduling the Computation<a class="headerlink" href="#scheduling-the-computation" title="永久链接至标题">#</a></h2>
<p>We’ll look at a set of schedule transformations necessary to map the
2D convolution onto VTA in an efficient fashion.
Those include:</p>
<ul class="simple">
<li><p>Computation blocking</p></li>
<li><p>Virtual threading to increase compute utilization</p></li>
<li><p>Lowering to VTA hardware intrinsics</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create TVM schedule</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">create_schedule</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
<span class="c1"># Let&#39;s look at the default TVM schedule</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tvm</span><span class="o">.</span><span class="n">lower</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">data</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">res</span><span class="p">],</span> <span class="n">simple_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<section id="blocking-the-computation">
<h3>Blocking the Computation<a class="headerlink" href="#blocking-the-computation" title="永久链接至标题">#</a></h3>
<p>The 2D convolution is by default too large for activations or kernel weights
to fit on VTA’s on-chip buffers all at once.
We apply blocking along input channels, output channels, and along
the height spatial dimensions.
We don’t apply blocking along the width spatial dimension since it’s
the innermost dimension in the NCHW layout (and consequently to increase
locality, it’s best not to block along the innermost dimension).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s define tiling sizes</span>
<span class="n">b_block</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">//</span> <span class="n">env</span><span class="o">.</span><span class="n">BATCH</span>
<span class="n">oc_block</span> <span class="o">=</span> <span class="mi">128</span> <span class="o">//</span> <span class="n">env</span><span class="o">.</span><span class="n">BLOCK_OUT</span>
<span class="n">ic_block</span> <span class="o">=</span> <span class="mi">16</span> <span class="o">//</span> <span class="n">env</span><span class="o">.</span><span class="n">BLOCK_IN</span>
<span class="n">h_block</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">w_block</span> <span class="o">=</span> <span class="mi">14</span>

<span class="c1"># Tile the output tensor along the spatial and output channel dimensions</span>
<span class="c1"># (since by default we are doing single batch inference, the split along</span>
<span class="c1">#  the batch dimension has no effect)</span>
<span class="n">b</span><span class="p">,</span> <span class="n">oc</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">b_tns</span><span class="p">,</span> <span class="n">oc_tns</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">res</span><span class="p">]</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">axis</span>
<span class="n">b_out</span><span class="p">,</span> <span class="n">b_inn</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">res</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="n">b_block</span><span class="p">)</span>
<span class="n">oc_out</span><span class="p">,</span> <span class="n">oc_inn</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">res</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">oc</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="n">oc_block</span><span class="p">)</span>
<span class="n">y_out</span><span class="p">,</span> <span class="n">y_inn</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">res</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="n">h_block</span><span class="p">)</span>
<span class="n">x_out</span><span class="p">,</span> <span class="n">x_inn</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">res</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="n">w_block</span><span class="p">)</span>
<span class="n">s</span><span class="p">[</span><span class="n">res</span><span class="p">]</span><span class="o">.</span><span class="n">reorder</span><span class="p">(</span><span class="n">b_out</span><span class="p">,</span> <span class="n">oc_out</span><span class="p">,</span> <span class="n">y_out</span><span class="p">,</span> <span class="n">x_out</span><span class="p">,</span> <span class="n">b_inn</span><span class="p">,</span> <span class="n">oc_inn</span><span class="p">,</span> <span class="n">y_inn</span><span class="p">,</span> <span class="n">x_inn</span><span class="p">,</span> <span class="n">b_tns</span><span class="p">,</span> <span class="n">oc_tns</span><span class="p">)</span>

<span class="c1"># Move intermediate computation into each output compute tile</span>
<span class="n">s</span><span class="p">[</span><span class="n">res_conv</span><span class="p">]</span><span class="o">.</span><span class="n">compute_at</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">res</span><span class="p">],</span> <span class="n">x_out</span><span class="p">)</span>
<span class="n">s</span><span class="p">[</span><span class="n">res_shr</span><span class="p">]</span><span class="o">.</span><span class="n">compute_at</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">res</span><span class="p">],</span> <span class="n">x_out</span><span class="p">)</span>
<span class="n">s</span><span class="p">[</span><span class="n">res_max</span><span class="p">]</span><span class="o">.</span><span class="n">compute_at</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">res</span><span class="p">],</span> <span class="n">x_out</span><span class="p">)</span>
<span class="n">s</span><span class="p">[</span><span class="n">res_min</span><span class="p">]</span><span class="o">.</span><span class="n">compute_at</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">res</span><span class="p">],</span> <span class="n">x_out</span><span class="p">)</span>

<span class="c1"># Apply additional loop split along reduction axis (input channel)</span>
<span class="n">b_inn</span><span class="p">,</span> <span class="n">oc_inn</span><span class="p">,</span> <span class="n">y_inn</span><span class="p">,</span> <span class="n">x_inn</span><span class="p">,</span> <span class="n">b_tns</span><span class="p">,</span> <span class="n">oc_tns</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">res_conv</span><span class="p">]</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">axis</span>
<span class="n">ic_out</span><span class="p">,</span> <span class="n">ic_inn</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">res_conv</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">ic</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="n">ic_block</span><span class="p">)</span>

<span class="c1"># Reorder axes.</span>
<span class="c1"># 1) Group the VTA tensor axes in the inner most position: b_tns, oc_tns, ic_tns</span>
<span class="c1">#    to allow TVM to tensorize.</span>
<span class="c1"># 2) We move the ic_out axis all the way out of the convolution loop to block</span>
<span class="c1">#    along the reduction axis.</span>
<span class="c1"># 3) Now we re-order the block axes: b_inn, oc_inn, y_inn, x_inn, ic_inn, dy, dx.</span>
<span class="c1">#    VTA runtime/hardware requires us to write to a different output feature map</span>
<span class="c1">#    location for every VTA tensor operation.</span>
<span class="c1">#    This restriction requires us to order one of oc_inn, y_inn or x_inn right</span>
<span class="c1">#    before b_tns, since they all affect output feature map indexing.</span>
<span class="c1">#    Therefore, we choose to bring x_inn inside as shown below.</span>
<span class="n">s</span><span class="p">[</span><span class="n">res_conv</span><span class="p">]</span><span class="o">.</span><span class="n">reorder</span><span class="p">(</span><span class="n">ic_out</span><span class="p">,</span> <span class="n">b_inn</span><span class="p">,</span> <span class="n">oc_inn</span><span class="p">,</span> <span class="n">y_inn</span><span class="p">,</span> <span class="n">ic_inn</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">x_inn</span><span class="p">,</span> <span class="n">b_tns</span><span class="p">,</span> <span class="n">oc_tns</span><span class="p">,</span> <span class="n">ic_tns</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="virtual-threading">
<h3>Virtual Threading<a class="headerlink" href="#virtual-threading" title="永久链接至标题">#</a></h3>
<p>Virtual threading is a mechanism that increases task-level pipeline
parallelism in the VTA hardware design.
Put it another way, it increases compute resource utilization by hiding
memory access latency.</p>
<p>In the implementation below, virtual threading distributes work across two
threads split along the output channel axis.
We show how work is split when computing the 2D convolution in the figure
below.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/uwsampl/web-data/main/vta/tutorial/virtual_threading.png"><img alt="https://raw.githubusercontent.com/uwsampl/web-data/main/vta/tutorial/virtual_threading.png" class="align-center" src="https://raw.githubusercontent.com/uwsampl/web-data/main/vta/tutorial/virtual_threading.png" style="width: 480px;" /></a>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># VTA only supports 2 virtual threads</span>
<span class="n">v_threads</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Perform virtual thread split along output channel outer axis</span>
<span class="n">_</span><span class="p">,</span> <span class="n">tx</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">res</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">oc_out</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="n">v_threads</span><span class="p">)</span>
<span class="n">s</span><span class="p">[</span><span class="n">res</span><span class="p">]</span><span class="o">.</span><span class="n">reorder</span><span class="p">(</span><span class="n">tx</span><span class="p">,</span> <span class="n">b_out</span><span class="p">)</span>
<span class="n">s</span><span class="p">[</span><span class="n">res</span><span class="p">]</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">tx</span><span class="p">,</span> <span class="n">te</span><span class="o">.</span><span class="n">thread_axis</span><span class="p">(</span><span class="s2">&quot;cthread&quot;</span><span class="p">))</span>

<span class="c1"># Let&#39;s look at the current TVM schedule after blocking and virtual threading</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tvm</span><span class="o">.</span><span class="n">lower</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">data</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">res</span><span class="p">],</span> <span class="n">simple_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="lowering-copies-to-dma-transfers">
<h3>Lowering Copies to DMA Transfers<a class="headerlink" href="#lowering-copies-to-dma-transfers" title="永久链接至标题">#</a></h3>
<p>Next we set the buffer scopes to the corresponding on-chip VTA SRAM buffers.
We move the load loops into the 2D convolution computation loop to stage
memory loads such that they fit in the on-chip SRAM buffers.
Finally we annotate the load/store loop outer axes with the DMA copy pragma
to perform bulk memory transfers on VTA.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set scope of SRAM buffers</span>
<span class="n">s</span><span class="p">[</span><span class="n">data_buf</span><span class="p">]</span><span class="o">.</span><span class="n">set_scope</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">inp_scope</span><span class="p">)</span>
<span class="n">s</span><span class="p">[</span><span class="n">kernel_buf</span><span class="p">]</span><span class="o">.</span><span class="n">set_scope</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">wgt_scope</span><span class="p">)</span>
<span class="n">s</span><span class="p">[</span><span class="n">res_conv</span><span class="p">]</span><span class="o">.</span><span class="n">set_scope</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">acc_scope</span><span class="p">)</span>
<span class="n">s</span><span class="p">[</span><span class="n">res_shr</span><span class="p">]</span><span class="o">.</span><span class="n">set_scope</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">acc_scope</span><span class="p">)</span>
<span class="n">s</span><span class="p">[</span><span class="n">res_min</span><span class="p">]</span><span class="o">.</span><span class="n">set_scope</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">acc_scope</span><span class="p">)</span>
<span class="n">s</span><span class="p">[</span><span class="n">res_max</span><span class="p">]</span><span class="o">.</span><span class="n">set_scope</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">acc_scope</span><span class="p">)</span>

<span class="c1"># Block data and kernel cache reads</span>
<span class="n">s</span><span class="p">[</span><span class="n">data_buf</span><span class="p">]</span><span class="o">.</span><span class="n">compute_at</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">res_conv</span><span class="p">],</span> <span class="n">ic_out</span><span class="p">)</span>
<span class="n">s</span><span class="p">[</span><span class="n">kernel_buf</span><span class="p">]</span><span class="o">.</span><span class="n">compute_at</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">res_conv</span><span class="p">],</span> <span class="n">ic_out</span><span class="p">)</span>

<span class="c1"># Use DMA copy pragma on DRAM-&gt;SRAM operations</span>
<span class="n">s</span><span class="p">[</span><span class="n">data_buf</span><span class="p">]</span><span class="o">.</span><span class="n">pragma</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">data_buf</span><span class="p">]</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">axis</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">env</span><span class="o">.</span><span class="n">dma_copy</span><span class="p">)</span>
<span class="n">s</span><span class="p">[</span><span class="n">kernel_buf</span><span class="p">]</span><span class="o">.</span><span class="n">pragma</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">kernel_buf</span><span class="p">]</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">axis</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">env</span><span class="o">.</span><span class="n">dma_copy</span><span class="p">)</span>

<span class="c1"># Use DMA copy pragma on SRAM-&gt;DRAM operation in each result block</span>
<span class="c1"># (this implies that these copies should be performed along b_inn,</span>
<span class="c1"># or result axis 4)</span>
<span class="n">s</span><span class="p">[</span><span class="n">res</span><span class="p">]</span><span class="o">.</span><span class="n">pragma</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">res</span><span class="p">]</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">axis</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">env</span><span class="o">.</span><span class="n">dma_copy</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="lowering-computation-to-vta-compute-intrinsics">
<h3>Lowering Computation to VTA Compute Intrinsics<a class="headerlink" href="#lowering-computation-to-vta-compute-intrinsics" title="永久链接至标题">#</a></h3>
<p>The last phase is to lower the computation loops down to VTA hardware
intrinsics by mapping the 2D convolution to tensor intrinsics,
and mapping the shift, and clipping computation to the vector ALU.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Apply tensorization over the batch tensor tile axis</span>
<span class="n">s</span><span class="p">[</span><span class="n">res_conv</span><span class="p">]</span><span class="o">.</span><span class="n">tensorize</span><span class="p">(</span><span class="n">b_tns</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">gemm</span><span class="p">)</span>

<span class="c1"># Add an ALU pragma over the shift and clipping operations</span>
<span class="n">s</span><span class="p">[</span><span class="n">res_shr</span><span class="p">]</span><span class="o">.</span><span class="n">pragma</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">res_shr</span><span class="p">]</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">axis</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">env</span><span class="o">.</span><span class="n">alu</span><span class="p">)</span>
<span class="n">s</span><span class="p">[</span><span class="n">res_min</span><span class="p">]</span><span class="o">.</span><span class="n">pragma</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">res_min</span><span class="p">]</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">axis</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">env</span><span class="o">.</span><span class="n">alu</span><span class="p">)</span>
<span class="n">s</span><span class="p">[</span><span class="n">res_max</span><span class="p">]</span><span class="o">.</span><span class="n">pragma</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">res_max</span><span class="p">]</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">axis</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">env</span><span class="o">.</span><span class="n">alu</span><span class="p">)</span>

<span class="c1"># Let&#39;s look at the final lowered TVM schedule after lowering memory</span>
<span class="c1"># loads/stores down to DMA copy intrinsics, and the computation down to</span>
<span class="c1"># VTA compute intrinsics.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vta</span><span class="o">.</span><span class="n">lower</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">data</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">res</span><span class="p">],</span> <span class="n">simple_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</section>
</section>
<section id="tvm-compilation-and-verification">
<h2>TVM Compilation and Verification<a class="headerlink" href="#tvm-compilation-and-verification" title="永久链接至标题">#</a></h2>
<p>After specifying the schedule, we can compile it into a TVM function.
We save the module so we can send it over RPC.
We run the function and verify it against a numpy implementation to
ensure correctness.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># This library facilitates 2D convolution testing</span>
<span class="kn">from</span> <span class="nn">tvm.topi.testing</span> <span class="kn">import</span> <span class="n">conv2d_nchw_python</span>

<span class="c1"># Compile the TVM module</span>
<span class="n">my_conv</span> <span class="o">=</span> <span class="n">vta</span><span class="o">.</span><span class="n">build</span><span class="p">(</span>
    <span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">data</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">res</span><span class="p">],</span> <span class="n">tvm</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">Target</span><span class="p">(</span><span class="s2">&quot;ext_dev&quot;</span><span class="p">,</span> <span class="n">host</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">target_host</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;my_conv&quot;</span>
<span class="p">)</span>
<span class="n">temp</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">tempdir</span><span class="p">()</span>
<span class="n">my_conv</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">temp</span><span class="o">.</span><span class="n">relpath</span><span class="p">(</span><span class="s2">&quot;conv2d.o&quot;</span><span class="p">))</span>
<span class="n">remote</span><span class="o">.</span><span class="n">upload</span><span class="p">(</span><span class="n">temp</span><span class="o">.</span><span class="n">relpath</span><span class="p">(</span><span class="s2">&quot;conv2d.o&quot;</span><span class="p">))</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">remote</span><span class="o">.</span><span class="n">load_module</span><span class="p">(</span><span class="s2">&quot;conv2d.o&quot;</span><span class="p">)</span>

<span class="c1"># Get the remote device context</span>
<span class="n">ctx</span> <span class="o">=</span> <span class="n">remote</span><span class="o">.</span><span class="n">ext_dev</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Initialize the data and kernel arrays randomly in the int range</span>
<span class="c1"># of (-128, 128] in NCHW layout</span>
<span class="n">data_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span>
    <span class="n">data</span><span class="o">.</span><span class="n">dtype</span>
<span class="p">)</span>
<span class="n">kernel_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span>
    <span class="o">-</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">kernel_h</span><span class="p">,</span> <span class="n">kernel_w</span><span class="p">)</span>
<span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">kernel</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

<span class="c1"># Apply packing to the data and kernel arrays from a 2D NCHW</span>
<span class="c1"># to a 4D NCHWnc packed layout</span>
<span class="n">data_packed</span> <span class="o">=</span> <span class="n">data_np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
    <span class="n">batch_size</span> <span class="o">//</span> <span class="n">env</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span> <span class="n">in_channels</span> <span class="o">//</span> <span class="n">env</span><span class="o">.</span><span class="n">BLOCK_IN</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">BLOCK_IN</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span>
<span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">kernel_packed</span> <span class="o">=</span> <span class="n">kernel_np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
    <span class="n">out_channels</span> <span class="o">//</span> <span class="n">env</span><span class="o">.</span><span class="n">BLOCK_OUT</span><span class="p">,</span>
    <span class="n">env</span><span class="o">.</span><span class="n">BLOCK_OUT</span><span class="p">,</span>
    <span class="n">in_channels</span> <span class="o">//</span> <span class="n">env</span><span class="o">.</span><span class="n">BLOCK_IN</span><span class="p">,</span>
    <span class="n">env</span><span class="o">.</span><span class="n">BLOCK_IN</span><span class="p">,</span>
    <span class="n">kernel_h</span><span class="p">,</span>
    <span class="n">kernel_w</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Format the input/output arrays with tvm.nd.array to the DLPack standard</span>
<span class="n">data_nd</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data_packed</span><span class="p">,</span> <span class="n">ctx</span><span class="p">)</span>
<span class="n">kernel_nd</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">kernel_packed</span><span class="p">,</span> <span class="n">ctx</span><span class="p">)</span>
<span class="n">res_nd</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_shape</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">ctx</span><span class="p">)</span>

<span class="c1"># Clear stats</span>
<span class="k">if</span> <span class="n">env</span><span class="o">.</span><span class="n">TARGET</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;sim&quot;</span><span class="p">,</span> <span class="s2">&quot;tsim&quot;</span><span class="p">]:</span>
    <span class="n">simulator</span><span class="o">.</span><span class="n">clear_stats</span><span class="p">()</span>

<span class="c1"># Invoke the module to perform the computation</span>
<span class="n">f</span><span class="p">(</span><span class="n">data_nd</span><span class="p">,</span> <span class="n">kernel_nd</span><span class="p">,</span> <span class="n">res_nd</span><span class="p">)</span>

<span class="c1"># Verify against numpy implementation</span>
<span class="n">res_ref</span> <span class="o">=</span> <span class="n">conv2d_nchw_python</span><span class="p">(</span>
    <span class="n">data_np</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">acc_dtype</span><span class="p">),</span>
    <span class="n">kernel_np</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">acc_dtype</span><span class="p">),</span>
    <span class="p">(</span><span class="n">stride_h</span><span class="p">,</span> <span class="n">stride_w</span><span class="p">),</span>
    <span class="p">(</span><span class="n">pad_h</span><span class="p">,</span> <span class="n">pad_w</span><span class="p">),</span>
<span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">acc_dtype</span><span class="p">)</span>
<span class="n">res_ref</span> <span class="o">=</span> <span class="n">res_ref</span> <span class="o">&gt;&gt;</span> <span class="n">env</span><span class="o">.</span><span class="n">INP_WIDTH</span>
<span class="n">res_ref</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">res_ref</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">inp_max</span><span class="p">)</span>
<span class="n">res_ref</span> <span class="o">=</span> <span class="n">res_ref</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">res_ref</span> <span class="o">=</span> <span class="n">res_ref</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
    <span class="p">(</span>
        <span class="n">batch_size</span> <span class="o">//</span> <span class="n">env</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span>
        <span class="n">env</span><span class="o">.</span><span class="n">BATCH</span><span class="p">,</span>
        <span class="n">out_channels</span> <span class="o">//</span> <span class="n">env</span><span class="o">.</span><span class="n">BLOCK_OUT</span><span class="p">,</span>
        <span class="n">env</span><span class="o">.</span><span class="n">BLOCK_OUT</span><span class="p">,</span>
        <span class="n">fout_height</span><span class="p">,</span>
        <span class="n">fout_width</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">tvm</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">res_ref</span><span class="p">,</span> <span class="n">res_nd</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

<span class="c1"># Print stats</span>
<span class="k">if</span> <span class="n">env</span><span class="o">.</span><span class="n">TARGET</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;sim&quot;</span><span class="p">,</span> <span class="s2">&quot;tsim&quot;</span><span class="p">]:</span>
    <span class="n">sim_stats</span> <span class="o">=</span> <span class="n">simulator</span><span class="o">.</span><span class="n">stats</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Execution statistics:&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">sim_stats</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="si">{:&lt;16}</span><span class="s2">: </span><span class="si">{:&gt;16}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Successful 2D convolution test!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="永久链接至标题">#</a></h2>
<p>This tutorial demonstrates how TVM scheduling primitives can be used to
lower 2D convolution onto hardware accelerator intrinsics, making
use of hardware specific optimizations, such as latency hiding with
virtual threading.</p>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="index.html" title="上一页 页">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">上一页</p>
            <p class="prev-next-title">Optimize Tensor Operators</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="matrix_multiply_opt.html" title="下一页 页">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">下一页</p>
        <p class="prev-next-title">Matrix Multiply Blocking</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By xinetzone<br/>
  
      &copy; Copyright 2022, xinetzone.<br/>
    Last updated on 2022-04-21, 06:02:04.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>