# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.9.dev282+gf54634c5d\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-01-11 13:20+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:13
msgid ""
"Click :ref:`here "
"<sphx_glr_download_how_to_deploy_models_deploy_prequantized.py>` to "
"download the full example code"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:22
msgid "Deploy a Framework-prequantized Model with TVM"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:23
msgid "**Author**: `Masahiro Masuda <https://github.com/masahi>`_"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:25
msgid ""
"This is a tutorial on loading models quantized by deep learning "
"frameworks into TVM. Pre-quantized model import is one of the "
"quantization support we have in TVM. More details on the quantization "
"story in TVM can be found `here <https://discuss.tvm.apache.org/t"
"/quantization-story/3920>`_."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:30
msgid ""
"Here, we demonstrate how to load and run models quantized by PyTorch, "
"MXNet, and TFLite. Once loaded, we can run compiled, quantized models on "
"any hardware TVM supports."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:35
msgid "First, necessary imports"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:56
msgid "Helper functions to run the demo"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:118
msgid ""
"A mapping from label to class name, to verify that the outputs from "
"models below are reasonable"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:130
msgid "Everyone's favorite cat image for demonstration"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:142
msgid "Deploy a quantized PyTorch Model"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:143
msgid ""
"First, we demonstrate how to load deep learning models quantized by "
"PyTorch, using our PyTorch frontend."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:146
msgid ""
"Please refer to the PyTorch static quantization tutorial below to learn "
"about their quantization workflow. "
"https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:150
msgid ""
"We use this function to quantize PyTorch models. In short, this function "
"takes a floating point model and converts it to uint8. The model is per-"
"channel quantized."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:173
msgid "Load quantization-ready, pretrained Mobilenet v2 model from torchvision"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:174
msgid ""
"We choose mobilenet v2 because this model was trained with quantization "
"aware training. Other models require a full post training calibration."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:187
msgid "Quantize, trace and run the PyTorch Mobilenet v2 model"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:188
msgid ""
"The details are out of scope for this tutorial. Please refer to the "
"tutorials on the PyTorch website to learn about quantization and jit."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:206
msgid "Convert quantized Mobilenet v2 to Relay-QNN using the PyTorch frontend"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:207
msgid ""
"The PyTorch frontend has support for converting a quantized PyTorch model"
" to an equivalent Relay module enriched with quantization-aware "
"operators. We call this representation Relay QNN dialect."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:211
msgid ""
"You can print the output from the frontend to see how quantized models "
"are represented."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:214
msgid ""
"You would see operators specific to quantization such as qnn.quantize, "
"qnn.dequantize, qnn.requantize, and qnn.conv2d etc."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:230
msgid "Compile and run the Relay module"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:231
msgid ""
"Once we obtained the quantized Relay module, the rest of the workflow is "
"the same as running floating point models. Please refer to other "
"tutorials for more details."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:235
msgid ""
"Under the hood, quantization specific operators are lowered to a sequence"
" of standard Relay operators before compilation."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:248
msgid "Compare the output labels"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:249
msgid "We should see identical labels printed."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:264
msgid ""
"However, due to the difference in numerics, in general the raw floating "
"point outputs are not expected to be identical. Here, we print how many "
"floating point output values are identical out of 1000 outputs from "
"mobilenet v2."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:278
msgid "Measure performance"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:279
msgid ""
"Here we give an example of how to measure performance of TVM compiled "
"models."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:294
msgid "We recommend this method for the following reasons:"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:296
msgid "Measurements are done in C++, so there is no Python overhead"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:297
msgid "It includes several warm up runs"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:298
msgid "The same method can be used to profile on remote devices (android etc.)."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:304
msgid ""
"Unless the hardware has special support for fast 8 bit instructions, "
"quantized models are not expected to be any faster than FP32 models. "
"Without fast 8 bit instructions, TVM does quantized convolution in 16 "
"bit, even if the model itself is 8 bit."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:308
msgid ""
"For x86, the best performance can be achieved on CPUs with AVX512 "
"instructions set. In this case, TVM utilizes the fastest available 8 bit "
"instructions for the given target. This includes support for the VNNI 8 "
"bit dot product instruction (CascadeLake or newer)."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:312
msgid "Moreover, the following general tips for CPU performance equally applies:"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:314
msgid ""
"Set the environment variable TVM_NUM_THREADS to the number of physical "
"cores"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:315
msgid ""
"Choose the best target for your hardware, such as \"llvm -mcpu=skylake-"
"avx512\" or \"llvm -mcpu=cascadelake\" (more CPUs with AVX512 would come "
"in the future)"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:321
msgid "Deploy a quantized MXNet Model"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:322
#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:328
msgid "TODO"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:327
msgid "Deploy a quantized TFLite Model"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:343
msgid ""
":download:`Download Python source code: deploy_prequantized.py "
"<deploy_prequantized.py>`"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:349
msgid ""
":download:`Download Jupyter notebook: deploy_prequantized.ipynb "
"<deploy_prequantized.ipynb>`"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_prequantized.rst:356
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""

