
<!DOCTYPE html>

<html lang="zh_CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>添加算子到 Relay &#8212; TVM  文档</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/default.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script src="../../../_static/translations.js"></script>
    <script src="../../../_static/design-tabs.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../../../_static/tvm-logo-square.png"/>
    <link rel="index" title="索引" href="../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../search.html" />
    <link rel="next" title="Adding a Compiler Pass to Relay" href="relay_add_pass.html" />
    <link rel="prev" title="Debugging TVM" href="debugging_tvm.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="zh_CN">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/tvm-logo-small.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">TVM  文档</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../README.html">
   TVM 文档
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../start.html">
   快速上手
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../install/index.html">
     安装 TVM
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../install/from_source.html">
       从源码安装
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
      <label for="toctree-checkbox-3">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../install/nnpack.html">
         NNPACK Contrib Installation
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../install/docker.html">
       Docker 镜像
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../install/nnpack.html">
       NNPACK Contrib Installation
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../contribute/index.html">
     贡献者指南
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
    <label for="toctree-checkbox-4">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../contribute/community.html">
       TVM 社区指南
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../contribute/pull_request.html">
       提交 Pull Request
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../contribute/code_review.html">
       Code Reviews
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../contribute/committer_guide.html">
       Committer Guide
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../contribute/document.html">
       Documentation
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../contribute/code_guide.html">
       Code Guide and Tips
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../contribute/git_howto.html">
       Git Usage Tips
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../contribute/ci.html">
       Using TVM’s CI
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../contribute/release_process.html">
       Release Process
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../contribute/error_handling.html">
       Error Handling Guide
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../user-guide.html">
   用户手册
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorial/index.html">
     用户指南
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
    <label for="toctree-checkbox-6">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorial/introduction.html">
       TVM 和模型优化的概述
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorial/tvmc_command_line_driver.html">
       用 TVMC 编译和优化模型
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorial/tvmc_python.html">
       开始使用 TVMC Python：TVM 的高级 API
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorial/autotvm_relay_x86.html">
       用 Python 接口编译和优化模型（AutoTVM）
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorial/tensor_expr_get_started.html">
       使用张量表达式处理算子
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorial/autotvm_matmul_x86.html">
       用调度模板和 AutoTVM 优化算子
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorial/auto_scheduler_matmul_x86.html">
       使用自动调度优化运算
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorial/tensor_ir_blitz_course.html">
       TensorIR 的突击课程
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorial/cross_compilation_and_rpc.html">
       交叉编译和RPC
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorial/relay_quick_start.html">
       编译深度学习模型的快速入门教程
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorial/intro_topi.html">
       TOPI 简介
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorial/uma.html">
       通过 UMA 使您的硬件加速器 TVM-ready
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../how_to/index.html">
     How To 指南
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../how_to/compile_models/index.html">
       编译深度学习模型
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
      <label for="toctree-checkbox-8">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/compile_models/from_pytorch.html">
         编译 PyTorch 模型
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/compile_models/from_tensorflow.html">
         Compile Tensorflow Models
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/compile_models/from_mxnet.html">
         编译 MXNet 模型
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/compile_models/from_onnx.html">
         Compile ONNX Models
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/compile_models/from_keras.html">
         Compile Keras Models
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/compile_models/from_tflite.html">
         Compile TFLite Models
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/compile_models/from_coreml.html">
         Compile CoreML Models
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/compile_models/from_darknet.html">
         Compile YOLO-V2 and YOLO-V3 in DarkNet Models
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/compile_models/from_caffe2.html">
         Compile Caffe2 Models
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/compile_models/from_oneflow.html">
         Compile OneFlow Models
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/compile_models/from_paddle.html">
         Compile PaddlePaddle Models
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../how_to/deploy/index.html">
       部署模型并集成到 TVM
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
      <label for="toctree-checkbox-9">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/deploy/cpp_deploy.html">
         使用 C++ API 部署 TVM Module
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/deploy/android.html">
         Deploy to Android
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/deploy/integrate.html">
         集成 TVM 到你的项目
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/deploy/hls.html">
         HLS Backend Example
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/deploy/arm_compute_lib.html">
         集成 Relay Arm
         <sup>
          ®
         </sup>
         计算库
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/deploy/tensorrt.html">
         Relay TensorRT Integration
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/deploy/vitis_ai.html">
         Vitis AI Integration
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/deploy/bnns.html">
         Relay BNNS Integration
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../how_to/deploy_models/index.html">
       部署深度学习模型
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
      <label for="toctree-checkbox-10">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/deploy_models/deploy_model_on_android.html">
         Deploy the Pretrained Model on Android
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/deploy_models/deploy_model_on_rasp.html">
         Deploy the Pretrained Model on Raspberry Pi
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/deploy_models/deploy_object_detection_pytorch.html">
         编译 PyTorch 目标检测模型
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/deploy_models/deploy_prequantized.html">
         使用 TVM 部署框架预量化模型
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/deploy_models/deploy_prequantized_tflite.html">
         Deploy a Framework-prequantized Model with TVM - Part 3 (TFLite)
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/deploy_models/deploy_quantized.html">
         在 CUDA 上部署已量化模型
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/deploy_models/deploy_sparse.html">
         Deploy a Hugging Face Pruned Model on CPU
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/deploy_models/deploy_ssd_gluoncv.html">
         部署 Single Shot Multibox Detector(SSD) 模型
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/deploy_models/deploy_model_on_nano.html">
         Deploy the Pretrained Model on Jetson Nano
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../how_to/work_with_relay/index.html">
       使用 Relay
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
      <label for="toctree-checkbox-11">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/work_with_relay/build_gcn.html">
         构建图卷积网络
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/work_with_relay/using_external_lib.html">
         在 Relay 中使用外部库
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/work_with_relay/using_pipeline_executor.html">
         在 Relay 中使用管道执行器
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/work_with_relay/using_relay_viz.html">
         使用 Relay Visualizer 可视化 Relay
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../how_to/work_with_schedules/index.html">
       使用 Tensor Expression 和 Schedules
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
      <label for="toctree-checkbox-12">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/work_with_schedules/schedule_primitives.html">
         TVM 中的调度原语
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/work_with_schedules/reduction.html">
         Reduction
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/work_with_schedules/intrin_math.html">
         Intrinsics and Math Functions
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/work_with_schedules/scan.html">
         Scan and Recurrent Kernel
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/work_with_schedules/extern_op.html">
         外部张量函数
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/work_with_schedules/tensorize.html">
         Use Tensorize to Leverage Hardware Intrinsics
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/work_with_schedules/tuple_inputs.html">
         Compute and Reduce with Tuple Inputs
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/work_with_schedules/tedd.html">
         使用 TEDD 进行可视化
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../how_to/optimize_operators/index.html">
       优化张量算子
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
      <label for="toctree-checkbox-13">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/optimize_operators/opt_gemm.html">
         How to optimize GEMM on CPU
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/optimize_operators/opt_conv_cuda.html">
         How to optimize convolution on GPU
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/optimize_operators/opt_conv_tensorcore.html">
         How to optimize convolution using TensorCores
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../how_to/tune_with_autotvm/index.html">
       Auto-Tune with Templates and AutoTVM
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
      <label for="toctree-checkbox-14">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/tune_with_autotvm/tune_conv2d_cuda.html">
         Tuning High Performance Convolution on NVIDIA GPUs
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/tune_with_autotvm/tune_relay_cuda.html">
         Auto-tuning a Convolutional Network for NVIDIA GPU
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/tune_with_autotvm/tune_relay_x86.html">
         Auto-tuning a Convolutional Network for x86 CPU
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/tune_with_autotvm/tune_relay_arm.html">
         Auto-tuning a Convolutional Network for ARM CPU
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/tune_with_autotvm/tune_relay_mobile_gpu.html">
         Auto-tuning a Convolutional Network for Mobile GPU
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../how_to/tune_with_autoscheduler/index.html">
       使用自动调度器进行无模板调度
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
      <label for="toctree-checkbox-15">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/tune_with_autoscheduler/tune_conv2d_layer_cuda.html">
         Auto-scheduling a Convolution Layer for GPU
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/tune_with_autoscheduler/tune_network_x86.html">
         Auto-scheduling a Neural Network for x86 CPU
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/tune_with_autoscheduler/tune_network_cuda.html">
         Auto-scheduling a Neural Network for NVIDIA GPU
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/tune_with_autoscheduler/tune_network_arm.html">
         Auto-scheduling a Neural Network for ARM CPU
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/tune_with_autoscheduler/tune_network_mali.html">
         Auto-scheduling a Neural Network for mali GPU
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/tune_with_autoscheduler/tune_sparse_x86.html">
         Auto-scheduling Sparse Matrix Multiplication on CPU with Custom Sketch Rule
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../how_to/work_with_microtvm/index.html">
       使用 microTVM
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
      <label for="toctree-checkbox-16">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/work_with_microtvm/micro_aot.html">
         microTVM Host-Driven AoT
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/work_with_microtvm/micro_autotune.html">
         使用 microTVM Autotuning
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/work_with_microtvm/micro_ethosu.html">
         在 bare metal Arm® Cortex®-M55 CPU 和 Ethos™-U55 NPU 上运行 TVM
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/work_with_microtvm/micro_reference_vm.html">
         microTVM 参考虚拟机
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/work_with_microtvm/micro_tflite.html">
         microTVM with TFLite Models
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/work_with_microtvm/micro_train.html">
         Training Vision Models for microTVM on Arduino
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/work_with_microtvm/micro_tvmc.html">
         Executing a Tiny Model with TVMC Micro
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../how_to/extend_tvm/index.html">
       拓展 TVM
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
      <label for="toctree-checkbox-17">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/extend_tvm/low_level_custom_pass.html">
         编写定制 Pass
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/extend_tvm/use_pass_infra.html">
         如何使用 TVM Pass Infra
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/extend_tvm/use_pass_instrument.html">
         如何使用 TVM Pass Instrument
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/extend_tvm/bring_your_own_datatypes.html">
         自定义 TVM 数据类型
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../how_to/profile/index.html">
       模型剖析
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
      <label for="toctree-checkbox-18">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../how_to/profile/papi.html">
         PAPI 快速上手
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../errors.html">
       处理 TVM 的错误
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../faq.html">
       常见问题
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../../../developer-guide.html">
   开发手册
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
  <label for="toctree-checkbox-19">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../tutorial/index.html">
     开发者教程
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
    <label for="toctree-checkbox-20">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../tutorial/codebase_walkthrough.html">
       TVM 代码库的实例演练
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="how_to.html">
     开发者指南
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
    <label for="toctree-checkbox-21">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3">
      <a class="reference internal" href="debugging_tvm.html">
       Debugging TVM
      </a>
     </li>
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="#">
       添加算子到 Relay
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="relay_add_pass.html">
       Adding a Compiler Pass to Relay
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="relay_bring_your_own_codegen.html">
       带你自己的 Codegen 到 TVM
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="pytest_target_parametrization.html">
       Python 目标参数化
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../arch/index.html">
   设计与架构
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
  <label for="toctree-checkbox-22">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../arch/runtime.html">
     TVM 运行时系统
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../arch/debugger.html">
     调试器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../arch/virtual_machine.html">
     将 VM 放入 TVM：Relay Virtual Machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../arch/introduction_to_module_serialization.html">
     模块序列化简介
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../arch/pass_infra.html">
     Pass Infrastructure
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../arch/device_target_interactions.html">
     Device/Target Interactions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../arch/inferbound.html">
     InferBound Pass
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../arch/hybrid_script.html">
     Hybrid 前端开发指南
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../arch/relay_intro.html">
     Relay IR 简介
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../arch/relay_op_strategy.html">
     Relay 算子策略
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../arch/convert_layout.html">
     Convert Layout Pass
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../arch/benchmark.html">
     基准性能日志格式
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../arch/frontend/tensorflow.html">
     TensorFlow Frontend
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../arch/security.html">
     安全指南
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../arch/microtvm_design.html">
     microTVM Design Document
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../arch/microtvm_project_api.html">
     microTVM Project API
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../arch/model_library_format.html">
     Model 库格式
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../topic-guides.html">
   主题指南
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
  <label for="toctree-checkbox-23">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../topic/microtvm/index.html">
     microTVM：裸机上的 TVM
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../topic/vta/index.html">
     VTA：通用张量加速器
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/>
    <label for="toctree-checkbox-24">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../topic/vta/install.html">
       VTA 安装指南
      </a>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../topic/vta/dev/index.html">
       VTA 设计和开发指南
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/>
      <label for="toctree-checkbox-25">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../topic/vta/dev/config.html">
         VTA 配置
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../topic/vta/dev/hardware.html">
         VTA 硬件指南
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../topic/vta/tutorials/index.html">
       VTA 教程
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/>
      <label for="toctree-checkbox-26">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../topic/vta/tutorials/matrix_multiply.html">
         简单的矩阵乘法
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../topic/vta/tutorials/frontend/index.html">
         编译深度学习模型
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../topic/vta/tutorials/optimize/index.html">
         优化 Tensor 算子
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../topic/vta/tutorials/autotvm/index.html">
         自动调优
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../reference-guide.html">
   参考指南
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" type="checkbox"/>
  <label for="toctree-checkbox-27">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../reference/langref/index.html">
     语言参考
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" type="checkbox"/>
    <label for="toctree-checkbox-28">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/langref/relay_expr.html">
       Relay 表达式
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/langref/relay_type.html">
       Relay 类型系统
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/langref/relay_adt.html">
       Algebraic Data Types in Relay
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/langref/relay_op.html">
       Relay 核心张量算子
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/langref/relay_pattern.html">
       Relay 中的模式匹配
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/langref/hybrid_script.html">
       Hybrid 前端语言参考
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../reference/api/python/index.html">
     Python API
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" type="checkbox"/>
    <label for="toctree-checkbox-29">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/runtime.html">
       tvm.runtime
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/ndarray.html">
       tvm.runtime.ndarray
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/error.html">
       tvm.error
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/ir/module.html">
       tvm.ir.module
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/ir/index.html">
       tvm.ir
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/target.html">
       tvm.target
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/tir.html">
       tvm.tir
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/te.html">
       tvm.te
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/driver.html">
       tvm.driver
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/relay/index.html">
       tvm.relay
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/relay/frontend.html">
       tvm.relay.frontend
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/relay/nn.html">
       tvm.relay.nn
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/relay/vision.html">
       tvm.relay.vision
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/relay/image.html">
       tvm.relay.image
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/relay/transform.html">
       tvm.relay.transform
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/relay/analysis.html">
       tvm.relay.analysis
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/relay/backend.html">
       tvm.relay.backend
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/relay/dataflow_pattern.html">
       tvm.relay.dataflow_pattern
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/relay/testing.html">
       tvm.relay.testing
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/autotvm.html">
       tvm.autotvm
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/auto_scheduler.html">
       tvm.auto_scheduler
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/rpc.html">
       tvm.rpc
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/micro.html">
       tvm.micro
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/contrib.html">
       tvm.contrib
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/graph_executor.html">
       tvm.contrib.graph_executor
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/topi.html">
       tvm.topi
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../reference/api/python/vta/index.html">
       vta
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../reference/api/links.html">
     其他 API
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../reference/publications.html">
     出版物
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../refs/index.html">
   参考
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" type="checkbox"/>
  <label for="toctree-checkbox-30">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../../refs/_ffi/index.html">
     <code class="docutils literal notranslate">
      <span class="pre">
       _ffi
      </span>
     </code>
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" type="checkbox"/>
    <label for="toctree-checkbox-31">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../refs/_ffi/base.html">
       <code class="docutils literal notranslate">
        <span class="pre">
         _ffi.base
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../refs/_ffi/libinfo.html">
       <code class="docutils literal notranslate">
        <span class="pre">
         _ffi.libinfo
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../refs/_ffi/object.html">
       <code class="docutils literal notranslate">
        <span class="pre">
         _ffi._ctypes.object
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../refs/_ffi/registry.html">
       <code class="docutils literal notranslate">
        <span class="pre">
         _ffi.registry
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../refs/_ffi/runtime_ctypes.html">
       <code class="docutils literal notranslate">
        <span class="pre">
         _ffi.runtime_ctypes
        </span>
       </code>
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../../_sources/docs/dev/how_to/relay_add_op.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> 导航
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#defining-an-attribute-node">
   1. 定义属性节点
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#writing-a-type-relation">
   2. 编写类型关系
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#relating-the-arity-and-attributes-to-an-operation">
   3. 将 Arity 和 Attributes 关联到运算
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#defining-the-compute-of-the-operation">
   4. 定义算子的计算
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hooking-up-compute-and-strategy-with-relay">
   5. 使用 Relay 挂钩计算和策略
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#creating-a-relay-call-node-and-exposing-a-python-hook">
   6. 创建 Relay 调用节点并暴露 Python 钩子
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#including-a-cleaner-python-api-hook">
   7. 包括更干净的 Python API 钩子
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#writing-unit-tests">
   8. 编写单元测试！
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#other-topics">
   其他主题
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-operators">
     梯度算子
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adding-a-gradient-in-python">
     在 Python 中添加梯度
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adding-a-gradient-in-c">
     在 C++ 中添加梯度
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>添加算子到 Relay</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> 导航 </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#defining-an-attribute-node">
   1. 定义属性节点
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#writing-a-type-relation">
   2. 编写类型关系
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#relating-the-arity-and-attributes-to-an-operation">
   3. 将 Arity 和 Attributes 关联到运算
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#defining-the-compute-of-the-operation">
   4. 定义算子的计算
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hooking-up-compute-and-strategy-with-relay">
   5. 使用 Relay 挂钩计算和策略
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#creating-a-relay-call-node-and-exposing-a-python-hook">
   6. 创建 Relay 调用节点并暴露 Python 钩子
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#including-a-cleaner-python-api-hook">
   7. 包括更干净的 Python API 钩子
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#writing-unit-tests">
   8. 编写单元测试！
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#other-topics">
   其他主题
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-operators">
     梯度算子
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adding-a-gradient-in-python">
     在 Python 中添加梯度
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adding-a-gradient-in-c">
     在 C++ 中添加梯度
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="adding-an-operator-to-relay">
<span id="relay-add-op"></span><h1>添加算子到 Relay<a class="headerlink" href="#adding-an-operator-to-relay" title="永久链接至标题">#</a></h1>
<p>在本文档中，将介绍在 Relay 中注册新的 TVM 算子所需的步骤。遵循这个 PR，它增加了 <a class="reference external" href="https://github.com/apache/tvm/pull/7722">cumulative product</a> 作为例子。PR 本身建立在另一个 PR 的基础上，后者添加了 <a class="reference external" href="https://github.com/apache/tvm/pull/7334">cumulative sum</a> 算子。</p>
<p>注册新的算子需要几个步骤：</p>
<ol class="arabic simple">
<li><p>添加属性节点，声明在编译时已知的固定参数</p></li>
<li><p>为集成到 Relay 类型系统中的运算编写类型关系。</p></li>
<li><p>使用 C++ 中的 <code class="docutils literal notranslate"><span class="pre">RELAY_REGISTER_OP</span></code> 宏为编译器注册算子的属性、类型和其他提示</p></li>
<li><p>编写算子的计算方式</p></li>
<li><p>注册 Relay 算子的 compute, schedule</p></li>
<li><p>定义 C++ 函数，为算子生成 call 节点，并为该函数注册 Python API 钩子</p></li>
<li><p>将上面的 Python API 钩子包装在更整洁的接口中</p></li>
<li><p>为新的 Relay 算子编写测试</p></li>
</ol>
<section id="defining-an-attribute-node">
<h2>1. 定义属性节点<a class="headerlink" href="#defining-an-attribute-node" title="永久链接至标题">#</a></h2>
<p>属性是固定的参数，应该在编译时就知道。卷积算子的 stride 和 expand 是属于卷积算子属性节点的字段的一个适当的例子。</p>
<p>属性应该定义在 <a class="reference external" href="https://github.com/apache/tvm/tree/main/include/tvm/relay/attrs">include/tvm/relay/attrs/</a> 文件夹下的文件中</p>
<p>最终希望创建一个算子，它的接口可以在最终的 python 接口中清楚地看到：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cumprod</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Numpy style cumprod op. Return the cumulative inclusive product of the elements along</span>
<span class="sd">    a given axis.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    data : relay.Expr</span>
<span class="sd">        The input data to the operator.</span>
<span class="sd">    axis : int, optional</span>
<span class="sd">        Axis along which the cumulative product is computed. The default (None) is to compute</span>
<span class="sd">        the cumprod over the flattened array.</span>
<span class="sd">    dtype : string, optional</span>
<span class="sd">        Type of the returned array and of the accumulator in which the elements are multiplied.</span>
<span class="sd">        If dtype is not specified, it defaults to the dtype of data.</span>
<span class="sd">    exclusive : bool, optional</span>
<span class="sd">        If true will return exclusive product in which the first element is not</span>
<span class="sd">        included. In other terms, if true, the j-th output element would be</span>
<span class="sd">        the product of the first (j-1) elements. Otherwise, it would be the product of</span>
<span class="sd">        the first j elements. The product of zero elements will be 1.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    result : relay.Expr</span>
<span class="sd">        The result has the same size as data, and the same shape as data if axis is not None.</span>
<span class="sd">        If axis is None, the result is a 1-d array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</pre></div>
</div>
<p>实现 <code class="docutils literal notranslate"><span class="pre">cumsum()</span></code> 类似的接口。</p>
<p>因此，当在 <code class="docutils literal notranslate"><span class="pre">include/tvm/relay/attrs/transform.h</span></code> 中定义属性时，选择算子的 axis、累积 dtype 和 exclusivity 作为结构的适当字段。</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cm">/*! \brief Attributes used in cumsum and cumprod operator */</span><span class="w"></span>
<span class="k">struct</span><span class="w"> </span><span class="nc">ScanopAttrs</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">tvm</span><span class="o">::</span><span class="n">AttrsNode</span><span class="o">&lt;</span><span class="n">ScanopAttrs</span><span class="o">&gt;</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="n">Integer</span><span class="w"> </span><span class="n">axis</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="n">DataType</span><span class="w"> </span><span class="n">dtype</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="n">Bool</span><span class="w"> </span><span class="n">exclusive</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Bool</span><span class="p">(</span><span class="nb">false</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="n">TVM_DECLARE_ATTRS</span><span class="p">(</span><span class="n">ScanopAttrs</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;relay.attrs.ScanopAttrs&quot;</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">TVM_ATTR_FIELD</span><span class="p">(</span><span class="n">axis</span><span class="p">).</span><span class="n">describe</span><span class="p">(</span><span class="s">&quot;The axis to operate over&quot;</span><span class="p">).</span><span class="n">set_default</span><span class="p">(</span><span class="n">NullValue</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span><span class="p">());</span><span class="w"></span>
<span class="w">    </span><span class="n">TVM_ATTR_FIELD</span><span class="p">(</span><span class="n">dtype</span><span class="p">).</span><span class="n">describe</span><span class="p">(</span><span class="s">&quot;Output data type&quot;</span><span class="p">).</span><span class="n">set_default</span><span class="p">(</span><span class="n">NullValue</span><span class="o">&lt;</span><span class="n">DataType</span><span class="o">&gt;</span><span class="p">());</span><span class="w"></span>
<span class="w">    </span><span class="n">TVM_ATTR_FIELD</span><span class="p">(</span><span class="n">exclusive</span><span class="p">)</span><span class="w"></span>
<span class="w">        </span><span class="p">.</span><span class="n">describe</span><span class="p">(</span><span class="s">&quot;The first element is not included&quot;</span><span class="p">)</span><span class="w"></span>
<span class="w">        </span><span class="p">.</span><span class="n">set_default</span><span class="p">(</span><span class="n">Bool</span><span class="p">(</span><span class="nb">false</span><span class="p">));</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">};</span><span class="w"></span>
</pre></div>
</div>
</section>
<section id="writing-a-type-relation">
<h2>2. 编写类型关系<a class="headerlink" href="#writing-a-type-relation" title="永久链接至标题">#</a></h2>
<p>为了实现注册算子的灵活性以及在 Relay 中表示类型时更大的表达性和粒度，使用输入和输出类型之间的关系对算子进行类型划分。这些关系表示为接受输入类型和输出类型列表（这些类型中的任何一个都可能是不完整的）的函数，并返回满足该关系的输入和输出类型列表。这包括可以在编译时静态确定的形状信息。从本质上讲，算子的关系除了计算输出类型外，还可以强制执行所有必要的类型规则（即通过检查输入类型）。</p>
<p>累积乘法算子 和 sum 算子的类型关系可以在 <code class="docutils literal notranslate"><span class="pre">src/relay/op/tensor/transform.cc</span></code> 中找到：</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">TVM_REGISTER_NODE_TYPE</span><span class="p">(</span><span class="n">ScanopAttrs</span><span class="p">);</span><span class="w"></span>
<span class="kt">bool</span><span class="w"> </span><span class="nf">ScanopRel</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Array</span><span class="o">&lt;</span><span class="n">Type</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">types</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">num_inputs</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">Attrs</span><span class="o">&amp;</span><span class="w"> </span><span class="n">attrs</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">TypeReporter</span><span class="o">&amp;</span><span class="w"> </span><span class="n">reporter</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="c1">// types: [data, output]</span>
<span class="w">    </span><span class="n">ICHECK_EQ</span><span class="p">(</span><span class="n">types</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Expects two types, one for the input and another for the output&quot;</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="o">*</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">types</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">as</span><span class="o">&lt;</span><span class="n">TensorTypeNode</span><span class="o">&gt;</span><span class="p">();</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">ICHECK</span><span class="p">(</span><span class="n">types</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">as</span><span class="o">&lt;</span><span class="n">IncompleteTypeNode</span><span class="o">&gt;</span><span class="p">())</span><span class="w"></span>
<span class="w">        </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Scanop: expect input type to be TensorType but get &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">types</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span><span class="w"></span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>

<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="o">*</span><span class="w"> </span><span class="n">param</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">attrs</span><span class="p">.</span><span class="n">as</span><span class="o">&lt;</span><span class="n">ScanopAttrs</span><span class="o">&gt;</span><span class="p">();</span><span class="w"></span>

<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">param</span><span class="o">-&gt;</span><span class="n">dtype</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">dtype</span><span class="p">.</span><span class="n">is_void</span><span class="p">())</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="o">-&gt;</span><span class="n">dtype</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">param</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">.</span><span class="n">defined</span><span class="p">())</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">reporter</span><span class="o">-&gt;</span><span class="n">Assign</span><span class="p">(</span><span class="n">types</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="w"> </span><span class="n">TensorType</span><span class="p">(</span><span class="n">data</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">));</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="k">auto</span><span class="w"> </span><span class="n">prod</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span><span class="w"></span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">data</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">            </span><span class="n">prod</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">prod</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">data</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">];</span><span class="w"></span>
<span class="w">        </span><span class="p">}</span><span class="w"></span>
<span class="w">        </span><span class="n">reporter</span><span class="o">-&gt;</span><span class="n">Assign</span><span class="p">(</span><span class="n">types</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="w"> </span><span class="n">TensorType</span><span class="p">({</span><span class="n">prod</span><span class="p">},</span><span class="w"> </span><span class="n">dtype</span><span class="p">));</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
</section>
<section id="relating-the-arity-and-attributes-to-an-operation">
<h2>3. 将 Arity 和 Attributes 关联到运算<a class="headerlink" href="#relating-the-arity-and-attributes-to-an-operation" title="永久链接至标题">#</a></h2>
<p>然后，注册新 ops 的名称，并用调用接口注解它们。C++ 中的 <code class="docutils literal notranslate"><span class="pre">RELAY_REGISTER_OP</span></code> 宏允许开发人员在 Relay 中指定关于算子的以下信息：</p>
<ul class="simple">
<li><p>Arity（参数数量）</p></li>
<li><p>位置参数的名称和描述</p></li>
<li><p>支持水平（1 表示内部 intrinsic；较高的数字表示较少的积分或外部支持的算子）”</p></li>
<li><p>算子的类型关系</p></li>
<li><p>优化运算时有用的其他注解。</p></li>
</ul>
<p>再一次把它添加到 <code class="docutils literal notranslate"><span class="pre">src/relay/op/tensor/transform.cc</span></code> 中：</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">RELAY_REGISTER_OP</span><span class="p">(</span><span class="s">&quot;cumsum&quot;</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="p">.</span><span class="n">describe</span><span class="p">(</span><span class="w"></span>
<span class="w">        </span><span class="sa">R</span><span class="s">&quot;</span><span class="dl">doc(</span><span class="s">Return the cumulative sum of the elements along a given axis.</span><span class="dl">)doc</span><span class="s">&quot;</span><span class="w"> </span><span class="n">TVM_ADD_FILELINE</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="p">.</span><span class="n">set_num_inputs</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">&quot;data&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Tensor&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;The input tensor.&quot;</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="p">.</span><span class="n">set_support_level</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="p">.</span><span class="n">add_type_rel</span><span class="p">(</span><span class="s">&quot;Cumsum&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">ScanopRel</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="p">.</span><span class="n">set_attr</span><span class="o">&lt;</span><span class="n">TOpPattern</span><span class="o">&gt;</span><span class="p">(</span><span class="s">&quot;TOpPattern&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">kOpaque</span><span class="p">);</span><span class="w"></span>

<span class="n">RELAY_REGISTER_OP</span><span class="p">(</span><span class="s">&quot;cumprod&quot;</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="p">.</span><span class="n">describe</span><span class="p">(</span><span class="w"></span>
<span class="w">        </span><span class="sa">R</span><span class="s">&quot;</span><span class="dl">doc(</span><span class="s">Return the cumulative product of the elements along a given axis.</span><span class="dl">)doc</span><span class="s">&quot;</span><span class="w"> </span><span class="n">TVM_ADD_FILELINE</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="p">.</span><span class="n">set_num_inputs</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">&quot;data&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Tensor&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;The input tensor.&quot;</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="p">.</span><span class="n">set_support_level</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="p">.</span><span class="n">add_type_rel</span><span class="p">(</span><span class="s">&quot;Cumprod&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">ScanopRel</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="p">.</span><span class="n">set_attr</span><span class="o">&lt;</span><span class="n">TOpPattern</span><span class="o">&gt;</span><span class="p">(</span><span class="s">&quot;TOpPattern&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">kOpaque</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>在这种情况下，<code class="docutils literal notranslate"><span class="pre">TOpPattern</span></code> 是对编译器关于算子的计算模式的提示，这可能对融合算子很有用。<code class="docutils literal notranslate"><span class="pre">kOpaque</span></code> 告诉 TVM 不要费心尝试融合这个算子。</p>
</section>
<section id="defining-the-compute-of-the-operation">
<h2>4. 定义算子的计算<a class="headerlink" href="#defining-the-compute-of-the-operation" title="永久链接至标题">#</a></h2>
<p>虽然已经为运算定义了接口，但仍然需要定义如何执行累积加法和乘法的实际计算。</p>
<p>编写此代码超出了本教程的范围。现在，假设有经过良好测试的运算计算实现。关于如何做到这一点的更多细节，建议查看关于 <a class="reference internal" href="../../tutorial/tensor_expr_get_started.html#tutorial-tensor-expr-get-started"><span class="std std-ref">张量表达式的教程</span></a>，<a class="reference internal" href="../../tutorial/intro_topi.html#tutorial-topi"><span class="std std-ref">TVM 的算子目录(topi)</span></a>，并查看示例累积加法和乘法实现在 <a class="reference external" href="https://github.com/apache/tvm/blob/main/python/tvm/topi/scan.py">python/tvm/topi/scan.py</a> 和 gpu 版本在 <a class="reference external" href="https://github.com/apache/tvm/blob/main/python/tvm/topi/cuda/scan.py">python/tvm/topi/cuda/scan.py</a> 中找到。</p>
</section>
<section id="hooking-up-compute-and-strategy-with-relay">
<h2>5. 使用 Relay 挂钩计算和策略<a class="headerlink" href="#hooking-up-compute-and-strategy-with-relay" title="永久链接至标题">#</a></h2>
<p>在实现了计算函数之后，现在需要将其粘贴到 Relay 运算上。在 TVM 中，这不仅意味着定义计算，还意味着定义运算的调度。策略是一种选择要使用哪个计算和哪个调度的方法。例如，对于二维卷积，可能会认为是在进行 depthwise 卷积并分配到更有效的计算和调度结果。然而在我们的例子中，除了在我们的 CPU 和 GPU 实现之间调度之外，我们没有这样的需求。在 <code class="docutils literal notranslate"><span class="pre">python/tvm/relay/op/strategy/generic.py</span></code> 和 <code class="docutils literal notranslate"><span class="pre">python/tvm/relay/op/strategy/cuda.py</span></code> 中，添加了以下策略：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">wrap_compute_scanop</span><span class="p">(</span><span class="n">topi_compute</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wrap scanop style topi compute&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_compute_scanop</span><span class="p">(</span><span class="n">attrs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">topi_compute</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">attrs</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="n">attrs</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">attrs</span><span class="o">.</span><span class="n">exclusive</span><span class="p">)]</span>

    <span class="k">return</span> <span class="n">_compute_scanop</span>


<span class="nd">@override_native_generic_func</span><span class="p">(</span><span class="s2">&quot;cumsum_strategy&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">cumsum_strategy</span><span class="p">(</span><span class="n">attrs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">out_type</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;cumsum generic strategy&quot;&quot;&quot;</span>
    <span class="n">strategy</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">OpStrategy</span><span class="p">()</span>
    <span class="n">strategy</span><span class="o">.</span><span class="n">add_implementation</span><span class="p">(</span>
        <span class="n">wrap_compute_scanop</span><span class="p">(</span><span class="n">topi</span><span class="o">.</span><span class="n">cumsum</span><span class="p">),</span>
        <span class="n">wrap_topi_schedule</span><span class="p">(</span><span class="n">topi</span><span class="o">.</span><span class="n">generic</span><span class="o">.</span><span class="n">schedule_extern</span><span class="p">),</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;cumsum.generic&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">strategy</span>


<span class="nd">@override_native_generic_func</span><span class="p">(</span><span class="s2">&quot;cumprod_strategy&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">cumprod_strategy</span><span class="p">(</span><span class="n">attrs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">out_type</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;cumprod generic strategy&quot;&quot;&quot;</span>
    <span class="n">strategy</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">OpStrategy</span><span class="p">()</span>
    <span class="n">strategy</span><span class="o">.</span><span class="n">add_implementation</span><span class="p">(</span>
        <span class="n">wrap_compute_scanop</span><span class="p">(</span><span class="n">topi</span><span class="o">.</span><span class="n">cumprod</span><span class="p">),</span>
        <span class="n">wrap_topi_schedule</span><span class="p">(</span><span class="n">topi</span><span class="o">.</span><span class="n">generic</span><span class="o">.</span><span class="n">schedule_extern</span><span class="p">),</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;cumprod.generic&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">strategy</span>

<span class="nd">@cumsum_strategy</span><span class="o">.</span><span class="n">register</span><span class="p">([</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="s2">&quot;gpu&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">cumsum_strategy_cuda</span><span class="p">(</span><span class="n">attrs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">out_type</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;cumsum cuda strategy&quot;&quot;&quot;</span>
    <span class="n">strategy</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">OpStrategy</span><span class="p">()</span>
    <span class="n">strategy</span><span class="o">.</span><span class="n">add_implementation</span><span class="p">(</span>
        <span class="n">wrap_compute_scanop</span><span class="p">(</span><span class="n">topi</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">cumsum</span><span class="p">),</span>
        <span class="n">wrap_topi_schedule</span><span class="p">(</span><span class="n">topi</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">schedule_scan</span><span class="p">),</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;cumsum.cuda&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">strategy</span>


<span class="nd">@cumprod_strategy</span><span class="o">.</span><span class="n">register</span><span class="p">([</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="s2">&quot;gpu&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">cumprod_strategy_cuda</span><span class="p">(</span><span class="n">attrs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">out_type</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;cumprod cuda strategy&quot;&quot;&quot;</span>
    <span class="n">strategy</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">OpStrategy</span><span class="p">()</span>
    <span class="n">strategy</span><span class="o">.</span><span class="n">add_implementation</span><span class="p">(</span>
        <span class="n">wrap_compute_scanop</span><span class="p">(</span><span class="n">topi</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">cumprod</span><span class="p">),</span>
        <span class="n">wrap_topi_schedule</span><span class="p">(</span><span class="n">topi</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">schedule_scan</span><span class="p">),</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;cumprod.cuda&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">strategy</span>
</pre></div>
</div>
<p>在每个策略中，我们在 <code class="docutils literal notranslate"><span class="pre">add_implementation()</span></code> 中定义编写的计算和要使用的调度。最后将策略和计算与 <code class="docutils literal notranslate"><span class="pre">python/tvm/relay/op/_transform.py</span></code> 中定义的 Relay 算子链接起来：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># cumsum</span>
<span class="nd">@_reg</span><span class="o">.</span><span class="n">register_compute</span><span class="p">(</span><span class="s2">&quot;cumsum&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">compute_cumsum</span><span class="p">(</span><span class="n">attrs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output_type</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute definition of cumsum&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">topi</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">attrs</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="n">attrs</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">attrs</span><span class="o">.</span><span class="n">exclusive</span><span class="p">)]</span>


<span class="n">_reg</span><span class="o">.</span><span class="n">register_strategy</span><span class="p">(</span><span class="s2">&quot;cumsum&quot;</span><span class="p">,</span> <span class="n">strategy</span><span class="o">.</span><span class="n">cumsum_strategy</span><span class="p">)</span>
<span class="n">_reg</span><span class="o">.</span><span class="n">register_shape_func</span><span class="p">(</span><span class="s2">&quot;cumsum&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">elemwise_shape_func</span><span class="p">)</span>

<span class="c1"># cumprod</span>
<span class="nd">@_reg</span><span class="o">.</span><span class="n">register_compute</span><span class="p">(</span><span class="s2">&quot;cumprod&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">compute_cumprod</span><span class="p">(</span><span class="n">attrs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output_type</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute definition of cumprod&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">topi</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">attrs</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="n">attrs</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">attrs</span><span class="o">.</span><span class="n">exclusive</span><span class="p">)]</span>


<span class="n">_reg</span><span class="o">.</span><span class="n">register_strategy</span><span class="p">(</span><span class="s2">&quot;cumprod&quot;</span><span class="p">,</span> <span class="n">strategy</span><span class="o">.</span><span class="n">cumprod_strategy</span><span class="p">)</span>
<span class="n">_reg</span><span class="o">.</span><span class="n">register_shape_func</span><span class="p">(</span><span class="s2">&quot;cumprod&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">elemwise_shape_func</span><span class="p">)</span>
</pre></div>
</div>
<p>在给定动态形状张量的情况下，形状函数用于确定输出形状。在这种情况下，告诉 TVM 输出形状将与输入形状相同。</p>
</section>
<section id="creating-a-relay-call-node-and-exposing-a-python-hook">
<h2>6. 创建 Relay 调用节点并暴露 Python 钩子<a class="headerlink" href="#creating-a-relay-call-node-and-exposing-a-python-hook" title="永久链接至标题">#</a></h2>
<p>有了可运行的运算，只需要通过 Relay 调用节点正确地调用它。这一步只需要编写一个函数，它接受算子的参数(作为 Relay 表达式)，并向 算子返回调用节点（即，应该放在对算子的调用所在的 Relay AST 中的节点）。</p>
<p>目前不支持调用属性和类型参数（最后两个字段），因此使用 <code class="docutils literal notranslate"><span class="pre">Op::Get</span></code> 从算子注册表中获取算子的信息并将参数传递给调用节点就足够了，如下所示。在 <code class="docutils literal notranslate"><span class="pre">src/relay/op/tensor/transform.cc</span></code>：</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">Expr</span><span class="w"> </span><span class="nf">MakeCumsum</span><span class="p">(</span><span class="n">Expr</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">Integer</span><span class="w"> </span><span class="n">axis</span><span class="p">,</span><span class="w"> </span><span class="n">DataType</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">Bool</span><span class="w"> </span><span class="n">exclusive</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">attrs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_object</span><span class="o">&lt;</span><span class="n">ScanopAttrs</span><span class="o">&gt;</span><span class="p">();</span><span class="w"></span>
<span class="w">    </span><span class="n">attrs</span><span class="o">-&gt;</span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dtype</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">attrs</span><span class="o">-&gt;</span><span class="n">axis</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">axis</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">attrs</span><span class="o">-&gt;</span><span class="n">exclusive</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">exclusive</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="k">static</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">Op</span><span class="o">&amp;</span><span class="w"> </span><span class="n">op</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Op</span><span class="o">::</span><span class="n">Get</span><span class="p">(</span><span class="s">&quot;cumsum&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">Call</span><span class="p">(</span><span class="n">op</span><span class="p">,</span><span class="w"> </span><span class="p">{</span><span class="n">data</span><span class="p">},</span><span class="w"> </span><span class="n">Attrs</span><span class="p">(</span><span class="n">attrs</span><span class="p">),</span><span class="w"> </span><span class="p">{});</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="n">TVM_REGISTER_GLOBAL</span><span class="p">(</span><span class="s">&quot;relay.op._make.cumsum&quot;</span><span class="p">).</span><span class="n">set_body_typed</span><span class="p">(</span><span class="n">MakeCumsum</span><span class="p">);</span><span class="w"></span>

<span class="n">Expr</span><span class="w"> </span><span class="nf">MakeCumprod</span><span class="p">(</span><span class="n">Expr</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">Integer</span><span class="w"> </span><span class="n">axis</span><span class="p">,</span><span class="w"> </span><span class="n">DataType</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">Bool</span><span class="w"> </span><span class="n">exclusive</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">attrs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_object</span><span class="o">&lt;</span><span class="n">ScanopAttrs</span><span class="o">&gt;</span><span class="p">();</span><span class="w"></span>
<span class="w">    </span><span class="n">attrs</span><span class="o">-&gt;</span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dtype</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">attrs</span><span class="o">-&gt;</span><span class="n">axis</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">axis</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">attrs</span><span class="o">-&gt;</span><span class="n">exclusive</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">exclusive</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="k">static</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">Op</span><span class="o">&amp;</span><span class="w"> </span><span class="n">op</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Op</span><span class="o">::</span><span class="n">Get</span><span class="p">(</span><span class="s">&quot;cumprod&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">Call</span><span class="p">(</span><span class="n">op</span><span class="p">,</span><span class="w"> </span><span class="p">{</span><span class="n">data</span><span class="p">},</span><span class="w"> </span><span class="n">Attrs</span><span class="p">(</span><span class="n">attrs</span><span class="p">),</span><span class="w"> </span><span class="p">{});</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="n">TVM_REGISTER_GLOBAL</span><span class="p">(</span><span class="s">&quot;relay.op._make.cumsum&quot;</span><span class="p">).</span><span class="n">set_body_typed</span><span class="p">(</span><span class="n">MakeCumprod</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>其中 <code class="docutils literal notranslate"><span class="pre">TVM_REGISTER_GLOBAL</span></code> 通过 <code class="docutils literal notranslate"><span class="pre">relay.op._make.cumsum(...)</span></code> 和 <code class="docutils literal notranslate"><span class="pre">relay.op._make.cumsum(...)</span></code> 在 Python 中公开 <code class="docutils literal notranslate"><span class="pre">MakeCumsum</span></code> 和 <code class="docutils literal notranslate"><span class="pre">MakeCumprod</span></code> 函数。</p>
</section>
<section id="including-a-cleaner-python-api-hook">
<h2>7. 包括更干净的 Python API 钩子<a class="headerlink" href="#including-a-cleaner-python-api-hook" title="永久链接至标题">#</a></h2>
<p>这通常是 Relay 中的约定，通过 <code class="docutils literal notranslate"><span class="pre">TVM_REGISTER_GLOBAL</span></code> 导出的函数应该包装在单独的 Python 函数中，而不是直接在 Python 中调用。对于我们的算子，在 <code class="docutils literal notranslate"><span class="pre">python/tvm/relay/op/transform.py</span></code> 中公开了这个更干净的接口</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cumsum</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_make</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">exclusive</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">cumprod</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_make</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">exclusive</span><span class="p">)</span>
</pre></div>
</div>
<p>注意，这些 Python 包装器也可能是为算子提供更简单接口的好机会。例如，<code class="docutils literal notranslate"><span class="pre">concat</span></code> 算子被注册为只接受一个算子，即一个包含要链接的张量的元组，但 Python 包装器将这些张量作为参数，并在生成调用节点之前将它们组合成一个元组：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">concat</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Concatenate the input tensors along the zero axis.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    args: list of Tensor</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    tensor: The concatenated tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tup</span> <span class="o">=</span> <span class="n">Tuple</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">_make</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">tup</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="writing-unit-tests">
<h2>8. 编写单元测试！<a class="headerlink" href="#writing-unit-tests" title="永久链接至标题">#</a></h2>
<p>这是不言自明的！一些单元测试示例可以在 <a class="reference external" href="https://github.com/apache/tvm/blob/main/tests/python/relay/test_op_level3.py">tests/python/relay/test_op_level3.py</a> 中找到，以获取累积加法和乘积算子。</p>
</section>
<section id="other-topics">
<h2>其他主题<a class="headerlink" href="#other-topics" title="永久链接至标题">#</a></h2>
<section id="gradient-operators">
<h3>梯度算子<a class="headerlink" href="#gradient-operators" title="永久链接至标题">#</a></h3>
<p>梯度算子对于在 Relay 中编写可微程序非常重要。虽然 Relay 的 autodiff 算法可以微分 first-class 语言结构，但算子是不透明的。</p>
<p>“Python 和 C++ 都可以用来编写梯度算子，但我们的例子集中在 Python 上，因为它更常用。”</p>
</section>
<section id="adding-a-gradient-in-python">
<h3>在 Python 中添加梯度<a class="headerlink" href="#adding-a-gradient-in-python" title="永久链接至标题">#</a></h3>
<p>Python 梯度算子的集合可以在 <code class="docutils literal notranslate"><span class="pre">python/tvm/relay/op/_tensor_grad.py</span></code> 中找到。介绍两个代表性的例子: <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> 和 <code class="docutils literal notranslate"><span class="pre">multiply</span></code>。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@register_gradient</span><span class="p">(</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sigmoid_grad</span><span class="p">(</span><span class="n">orig</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns [grad * sigmoid(x) * (1 - sigmoid(x))].&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">grad</span> <span class="o">*</span> <span class="n">orig</span> <span class="o">*</span> <span class="p">(</span><span class="n">ones_like</span><span class="p">(</span><span class="n">orig</span><span class="p">)</span> <span class="o">-</span> <span class="n">orig</span><span class="p">)]</span>
</pre></div>
</div>
<p>这里的输入是原始的算子 <code class="docutils literal notranslate"><span class="pre">orig</span></code> 和梯度 <code class="docutils literal notranslate"><span class="pre">grad</span></code> 进行累加。返回列表，其中第 i 个下标处的元素是算子对第 i 个输入的导数。一般来说，梯度函数会返回包含基本算子输入元素数量的列表。</p>
<p>在进一步分析这个定义之前，首先应该回忆一下 sigmoid 函数的导数： <span class="math notranslate nohighlight">\(\frac{\partial \sigma}{\partial x} = \sigma(x)(1 - \sigma(x))\)</span>。上面的定义看起来与数学定义相似，但有一个重要的补充，将在下面描述。</p>
<p>术语 <code class="docutils literal notranslate"><span class="pre">orig</span> <span class="pre">*</span> <span class="pre">(ones_like(orig)</span> <span class="pre">-</span> <span class="pre">orig)</span></code>  直接匹配导数，因为 <code class="docutils literal notranslate"><span class="pre">orig</span></code> 在这里是 sigmoid 函数，但我们感兴趣的不仅仅是如何计算这个函数的梯度。我们感兴趣的是将这个梯度与其他梯度组合起来，这样我们就可以在整个程序中累积梯度。这就是 <code class="docutils literal notranslate"><span class="pre">grad</span></code> 项的由来。在表达式 <code class="docutils literal notranslate"><span class="pre">grad</span> <span class="pre">*</span> <span class="pre">orig</span> <span class="pre">*</span> <span class="pre">(ones_like(orig)</span> <span class="pre">-</span> <span class="pre">orig)</span></code> 中，乘 <code class="docutils literal notranslate"><span class="pre">grad</span></code> 指定如何用迄今为止的梯度组合导数。</p>
<p>现在，我们考虑 <code class="docutils literal notranslate"><span class="pre">multiply</span></code>，一个稍微有趣一点的例子：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@register_gradient</span><span class="p">(</span><span class="s2">&quot;multiply&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">multiply_grad</span><span class="p">(</span><span class="n">orig</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns [grad * y, grad * x]&quot;&quot;&quot;</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">orig</span><span class="o">.</span><span class="n">args</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">collapse_sum_like</span><span class="p">(</span><span class="n">grad</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span>
            <span class="n">collapse_sum_like</span><span class="p">(</span><span class="n">grad</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)]</span>
</pre></div>
</div>
<p>在本例中，返回的列表中有两个元素，因为 <code class="docutils literal notranslate"><span class="pre">multiply</span></code> 是二元算子。回想一下，如果 <span class="math notranslate nohighlight">\(f(x, y) = xy\)</span>，偏导数是 <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x} = y\)</span> 和 <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial y} = x\)</span>。</p>
<p><code class="docutils literal notranslate"><span class="pre">multiply</span></code> 是 one required step，而 sigmoid 不需要，因为  <code class="docutils literal notranslate"><span class="pre">multiply</span></code> 具有广播语义。因为 <code class="docutils literal notranslate"><span class="pre">grad</span></code> 的形状可能与输入的形状不匹配，所以使用 <code class="docutils literal notranslate"><span class="pre">collapse_sum_like</span></code> 来获取 <code class="docutils literal notranslate"><span class="pre">grad</span> <span class="pre">*</span> <span class="pre">&lt;var&gt;</span></code> 项的内容，并使形状与我们要微分的输入的形状匹配。</p>
</section>
<section id="adding-a-gradient-in-c">
<h3>在 C++ 中添加梯度<a class="headerlink" href="#adding-a-gradient-in-c" title="永久链接至标题">#</a></h3>
<p>在 C++ 中添加梯度与在 Python 中添加梯度类似，但注册的接口略有不同。</p>
<p>首先，确保包含 <code class="docutils literal notranslate"><span class="pre">src/relay/transforms/pattern_utils.h</span></code>。它提供了在 Relay AST 中创建节点的辅助函数。然后，以与 Python 示例中类似的方式定义梯度：</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">tvm</span><span class="o">::</span><span class="n">Array</span><span class="o">&lt;</span><span class="n">Expr</span><span class="o">&gt;</span><span class="w"> </span><span class="n">MultiplyGrad</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Expr</span><span class="o">&amp;</span><span class="w"> </span><span class="n">orig_call</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">Expr</span><span class="o">&amp;</span><span class="w"> </span><span class="n">output_grad</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">Call</span><span class="o">&amp;</span><span class="w"> </span><span class="n">call</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">orig_call</span><span class="p">.</span><span class="n">Downcast</span><span class="o">&lt;</span><span class="n">Call</span><span class="o">&gt;</span><span class="p">();</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">CollapseSumLike</span><span class="p">(</span><span class="n">Multiply</span><span class="p">(</span><span class="n">output_grad</span><span class="p">,</span><span class="w"> </span><span class="n">call</span><span class="p">.</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span><span class="w"> </span><span class="n">call</span><span class="p">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="w"></span>
<span class="w">             </span><span class="n">CollapseSumLike</span><span class="p">(</span><span class="n">Multiply</span><span class="p">(</span><span class="n">output_grad</span><span class="p">,</span><span class="w"> </span><span class="n">call</span><span class="p">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="w"> </span><span class="n">call</span><span class="p">.</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="w"> </span><span class="p">};</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>注意，在 C++ 中，不能像在 Python 中那样使用相同的算子重载，而且需要向下强制转换，因此实现更加冗长。即便如此，可以很容易地验证这个定义与 Python 中前面的示例相对应。</p>
<p>现在，不使用 Python 装饰器，而是需要为 “FPrimalGradient” 在基本算子注册的末尾附加  <code class="docutils literal notranslate"><span class="pre">set_attr</span></code> 调用，以便注册梯度。</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">RELAY_REGISTER_OP</span><span class="p">(</span><span class="s">&quot;multiply&quot;</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="c1">// ...</span>
<span class="w">    </span><span class="c1">// Set other attributes</span>
<span class="w">    </span><span class="c1">// ...</span>
<span class="w">    </span><span class="p">.</span><span class="n">set_attr</span><span class="o">&lt;</span><span class="n">FPrimalGradient</span><span class="o">&gt;</span><span class="p">(</span><span class="s">&quot;FPrimalGradient&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">MultiplyGrad</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
</section>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="debugging_tvm.html" title="上一页 页">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">上一页</p>
            <p class="prev-next-title">Debugging TVM</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="relay_add_pass.html" title="下一页 页">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">下一页</p>
        <p class="prev-next-title">Adding a Compiler Pass to Relay</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By xinetzone<br/>
  
      &copy; Copyright 2022, xinetzone.<br/>
    Last updated on 2022-11-17, 06:26:12.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>