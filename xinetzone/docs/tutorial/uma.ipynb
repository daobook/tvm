{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 通过 UMA 使您的硬件加速器 TVM-ready\n",
        "\n",
        "**Authors**: [Michael J. Klaiber](https://github.com/MichaelJKlaiber), [Christoph Gerum](https://github.com/cgerum),\n",
        "[Paul Palomero Bernardo](https://github.com/PaulPalomeroBernardo/)\n",
        "\n",
        "这是 **通用模块化加速器接口** （Universal Modular Accelerator Interface，简称 UMA）的入门教程。UMA 提供简单易用的 API，将新的硬件加速器集成到 TVM 中。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "本教程逐步指导您如何使用 UMA，使您的硬件加速器 TVM-ready。\n",
        "\n",
        "虽然这个问题没有一刀切的解决方案，但 UMA 的目标是提供一个稳定的、仅使用 Python 的 API，将许多硬件加速器类集成到 TVM 中。\n",
        "\n",
        "在本教程中，您将了解 UMA API 在三个日益复杂的用例中。在这些用例中，三个模拟加速器 **Vanilla**、**Strawberry** 和 **Chocolate** 被引入并使用 UMA 集成到 TVM 中。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vanilla\n",
        "**Vanilla** is a simple accelerator consisting of a MAC array and has no internal memory.\n",
        "It is can ONLY process Conv2D layers, all other layers are executed on a CPU, that also orchestrates **Vanilla**.\n",
        "Both the CPU and Vanilla use a shared memory.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://raw.githubusercontent.com/apache/tvm-site/main/images/tutorial/uma_vanilla_block_diagram.png\" width=\"100%\" alt=\"A block diagram of Vanilla\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Vanilla** has a C interface ``vanilla_conv2dnchw(...)``` for carrying out a Conv2D operation (including same-padding),\n",
        "that accepts pointers to input feature map, weights and result,\n",
        "as well as the dimensions of `Conv2D`: `oc`, `iw`, `ih`, `ic`, `kh`, `kw`.\n",
        "\n",
        "```c++\n",
        "int vanilla_conv2dnchw(float* ifmap, float*  weights, float*  result, int oc, int iw, int ih, int ic, int kh, int kw);\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The script `uma_cli` creates code skeletons with API-calls into the UMA-API for new accelerators.\n",
        "\n",
        "For **Vanilla** we use it as follows: (``--tutorial vanilla`` adds all the additional files required for this part of the tutorial)\n",
        "\n",
        "```bash\n",
        "pip install inflection\n",
        "cd $TVM_HOME/apps/uma\n",
        "python uma_cli.py --add_hardware vanilla_accelerator --tutorial vanilla\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "uma_cli.py generates these files in the directory ``vanilla_accelerator`` which we are going to revist.\n",
        "\n",
        "```bash\n",
        "backend.py\n",
        "codegen.py\n",
        "conv2dnchw.cc\n",
        "passes.py\n",
        "patterns.py\n",
        "run.py\n",
        "strategies.py\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vanilla backend\n",
        "\n",
        " The generated backend for vanilla is found in `vanilla_accelerator/backend.py`:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "class VanillaAcceleratorBackend(UMABackend):\n",
        "    \"\"\"UMA backend for VanillaAccelerator.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self._register_pattern(\"conv2d\", conv2d_pattern())\n",
        "        self._register_tir_pass(PassPhase.TIR_PHASE_0, VanillaAcceleratorConv2DPass())\n",
        "        self._register_codegen(fmt=\"c\", includes=gen_includes)\n",
        "\n",
        "    @property\n",
        "    def target_name(self):\n",
        "        return \"vanilla_accelerator\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define offloaded patterns\n",
        "\n",
        "To specify that `Conv2D` is offloaded to **Vanilla**, it is described as Relay dataflow pattern\n",
        "([DFPattern](https://tvm.apache.org/docs/reference/langref/relay_pattern.html)) in `vanilla_accelerator/patterns.py`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "def conv2d_pattern():\n",
        "    pattern = is_op(\"nn.conv2d\")(wildcard(), wildcard())\n",
        "    pattern = pattern.has_attr({\"strides\": [1, 1]})\n",
        "    return pattern\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To map **Conv2D** operations from the input graph  to **Vanilla**'s\n",
        "low level function call ``vanilla_conv2dnchw(...)``, the TIR pass\n",
        "*VanillaAcceleratorConv2DPass* (that will be discussed later in this tutorial)\n",
        "is registered in `VanillaAcceleratorBackend`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Codegen\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The file ``vanilla_accelerator/codegen.py`` defines static  C-code that is added to the\n",
        "resulting C-Code generated by TVMś C-Codegen in ``gen_includes``.\n",
        "Here C-code is added to include **Vanilla**'s low level library``vanilla_conv2dnchw()``.\n",
        "\n",
        "```python\n",
        "def gen_includes() -> str:\n",
        "    topdir = pathlib.Path(__file__).parent.absolute()\n",
        "\n",
        "    includes = \"\"\n",
        "    includes += f'#include \"{topdir}/conv2dnchw.cc\"'\n",
        "    return includes\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As shown above in `VanillaAcceleratorBackend` it is registered to UMA with\n",
        "the `self._register_codegen`\n",
        "\n",
        "```python\n",
        "self._register_codegen(fmt=\"c\", includes=gen_includes)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Building the Neural Network and run it on Vanilla\n",
        "\n",
        "To demonstrate UMA's functionality, we will generate C code for a single Conv2D layer and run it on\n",
        "the Vanilla accelerator.\n",
        "The file ``vanilla_accelerator/run.py`` provides a demo running a Conv2D layer\n",
        "making use of Vanilla's C-API.\n",
        "\n",
        "\n",
        "```python\n",
        "def main():\n",
        "    mod, inputs, output_list, runner = create_conv2d()\n",
        "\n",
        "    uma_backend = VanillaAcceleratorBackend()\n",
        "    uma_backend.register()\n",
        "    mod = uma_backend.partition(mod)\n",
        "    target = tvm.target.Target(\"vanilla_accelerator\", host=tvm.target.Target(\"c\"))\n",
        "\n",
        "    export_directory = tvm.contrib.utils.tempdir(keep_for_debug=True).path\n",
        "    print(f\"Generated files are in {export_directory}\")\n",
        "    compile_and_run(\n",
        "        AOTModel(module=mod, inputs=inputs, outputs=output_list),\n",
        "        runner,\n",
        "        interface_api=\"c\",\n",
        "        use_unpacked_api=True,\n",
        "        target=target,\n",
        "        test_dir=str(export_directory),\n",
        "    )\n",
        "\n",
        "\n",
        "main()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By running ``vanilla_accelerator/run.py`` the output files are generated in the model library format (MLF).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```bash\n",
        "Generated files are in /tmp/tvm-debug-mode-tempdirs/2022-07-13T13-26-22___x5u76h0p/00000\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's examine the generated files:\n",
        "\n",
        "\n",
        "Output:\n",
        "\n",
        "```bash\n",
        "cd /tmp/tvm-debug-mode-tempdirs/2022-07-13T13-26-22___x5u76h0p/00000\n",
        "cd build/\n",
        "ls -1\n",
        "\n",
        "codegen\n",
        "lib.tar\n",
        "metadata.json\n",
        "parameters\n",
        "runtime\n",
        "src\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To evaluate the generated C code go to ``codegen/host/src/default_lib2.c``\n",
        "\n",
        "```bash\n",
        "cd codegen/host/src/\n",
        "ls -1\n",
        "\n",
        "default_lib0.c\n",
        "default_lib1.c\n",
        "default_lib2.c\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In `default_lib2.c` you can now see that the generated code calls\n",
        "into Vanilla's C-API and executes a Conv2D layer:\n",
        "\n",
        "```c++\n",
        "TVM_DLL int32_t tvmgen_default_vanilla_accelerator_main_0(float* placeholder, float* placeholder1, float* conv2d_nchw, uint8_t* global_workspace_1_var) {\n",
        "     vanilla_accelerator_conv2dnchw(placeholder, placeholder1, conv2d_nchw, 32, 14, 14, 32, 3, 3);\n",
        "     return 0;\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Strawberry\n",
        "Coming soon ...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chocolate\n",
        "Coming soon ...\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Request for Community Input\n",
        "If this tutorial **did not** fit to your accelerator, lease add your requirements to the UMA thread in\n",
        "the TVM discuss forum: [Link](https://discuss.tvm.apache.org/t/rfc-uma-universal-modular-accelerator-interface/12039).\n",
        "We are eager to extend this tutorial to provide guidance on making further classes of AI hardware\n",
        "accelerators TVM-ready using the UMA interface.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "[UMA-RFC] [UMA: Universal Modular Accelerator Interface](https://github.com/apache/tvm-rfcs/blob/main/rfcs/0060_UMA_Unified_Modular_Accelerator_Interface.md),\n",
        "TVM RFC, June 2022.\n",
        "\n",
        "[DFPattern] [Pattern Matching in Relay](https://tvm.apache.org/docs/reference/langref/relay_pattern.html)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.13 ('py38': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "28558e8daad512806f5c536a1a04c119185f99f65b79002708a12162d02a79c7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
