# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the TVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TVM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-01-13 14:28+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../../user-guide/tutorial/relay_quick_start.ipynb:20004
msgid "Quick Start Tutorial for Compiling Deep Learning Models"
msgstr ""

#: ../../user-guide/tutorial/relay_quick_start.ipynb:20005
msgid ""
"**Author**: `Yao Wang <https://github.com/kevinthesun>`_, `Truman Tian "
"<https://github.com/SiNZeRo>`_"
msgstr ""

#: ../../user-guide/tutorial/relay_quick_start.ipynb:20007
msgid ""
"This example shows how to build a neural network with Relay python "
"frontend and generates a runtime library for Nvidia GPU with TVM. Notice "
"that you need to build TVM with cuda and llvm enabled."
msgstr ""

#: ../../user-guide/tutorial/relay_quick_start.ipynb:30002
msgid "Overview for Supported Hardware Backend of TVM"
msgstr ""

#: ../../user-guide/tutorial/relay_quick_start.ipynb:30003
msgid "The image below shows hardware backend currently supported by TVM:"
msgstr ""

#: ../../user-guide/tutorial/relay_quick_start.ipynb:30007
msgid ""
"In this tutorial, we'll choose cuda and llvm as target backends. To begin"
" with, let's import Relay and TVM."
msgstr ""

#: ../../user-guide/tutorial/relay_quick_start.ipynb:50002
msgid "Define Neural Network in Relay"
msgstr ""

#: ../../user-guide/tutorial/relay_quick_start.ipynb:50003
msgid ""
"First, let's define a neural network with relay python frontend. For "
"simplicity, we'll use pre-defined resnet-18 network in Relay. Parameters "
"are initialized with Xavier initializer. Relay also supports other model "
"formats such as MXNet, CoreML, ONNX and Tensorflow."
msgstr ""

#: ../../user-guide/tutorial/relay_quick_start.ipynb:50009
msgid ""
"In this tutorial, we assume we will do inference on our device and the "
"batch size is set to be 1. Input images are RGB color images of size 224 "
"* 224. We can call the :py:meth:`tvm.relay.expr.TupleWrapper.astext()` to"
" show the network structure."
msgstr ""

#: ../../user-guide/tutorial/relay_quick_start.ipynb:70002
msgid "Compilation"
msgstr ""

#: ../../user-guide/tutorial/relay_quick_start.ipynb:70003
msgid ""
"Next step is to compile the model using the Relay/TVM pipeline. Users can"
" specify the optimization level of the compilation. Currently this value "
"can be 0 to 3. The optimization passes include operator fusion, pre-"
"computation, layout transformation and so on."
msgstr ""

#: ../../user-guide/tutorial/relay_quick_start.ipynb:70008
msgid ""
":py:func:`relay.build` returns three components: the execution graph in "
"json format, the TVM module library of compiled functions specifically "
"for this graph on the target hardware, and the parameter blobs of the "
"model. During the compilation, Relay does the graph-level optimization "
"while TVM does the tensor-level optimization, resulting in an optimized "
"runtime module for model serving."
msgstr ""

#: ../../user-guide/tutorial/relay_quick_start.ipynb:70015
msgid ""
"We'll first compile for Nvidia GPU. Behind the scene, "
":py:func:`relay.build` first does a number of graph-level optimizations, "
"e.g. pruning, fusing, etc., then registers the operators (i.e. the nodes "
"of the optimized graphs) to TVM implementations to generate a "
"`tvm.module`. To generate the module library, TVM will first transfer the"
" high level IR into the lower intrinsic IR of the specified target "
"backend, which is CUDA in this example. Then the machine code will be "
"generated as the module library."
msgstr ""

#: ../../user-guide/tutorial/relay_quick_start.ipynb:90002
msgid "Run the generate library"
msgstr ""

#: ../../user-guide/tutorial/relay_quick_start.ipynb:90003
msgid "Now we can create graph executor and run the module on Nvidia GPU."
msgstr ""

#: ../../user-guide/tutorial/relay_quick_start.ipynb:110002
msgid "Save and Load Compiled Module"
msgstr ""

#: ../../user-guide/tutorial/relay_quick_start.ipynb:110003
msgid ""
"We can also save the graph, lib and parameters into files and load them "
"back in deploy environment."
msgstr ""

