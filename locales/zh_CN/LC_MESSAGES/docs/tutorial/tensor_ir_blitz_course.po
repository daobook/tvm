# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the TVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TVM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-01-13 14:28+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:20004
msgid "Blitz Course to TensorIR"
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:20005
msgid "**Author**: `Siyuan Feng <https://github.com/Hzfengsy>`_"
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:20007
msgid ""
"TensorIR is a domain specific language for deep learning programs serving"
" two broad purposes:"
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:20009
msgid ""
"An implementation for transforming and optimizing programs on various "
"hardware backends."
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:20011
msgid "An abstraction for automatic tensorized program optimization."
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:40002
msgid "IRModule"
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:40003
msgid ""
"An IRModule is the central data structure in TVM, which contains deep "
"learning programs. It is the basic object of interest of IR "
"transformation and model building."
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:40008
msgid ""
"This is the life cycle of an IRModule, which can be created from "
"TVMScript. TensorIR schedule primitives and passes are two major ways to "
"transform an IRModule. Also, a sequence of transformations on an IRModule"
" is acceptable. Note that we can print an IRModule at **ANY** stage to "
"TVMScript. After all transformations and optimizations are complete, we "
"can build the IRModule to a runnable module to deploy on target devices."
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:40014
msgid ""
"Based on the design of TensorIR and IRModule, we are able to create a new"
" programming method:"
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:40016
msgid "Write a program by TVMScript in a python-AST based syntax."
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:40018
msgid "Transform and optimize a program with python api."
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:40020
msgid ""
"Interactively inspect and try the performance with an imperative style "
"transformation API."
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:50002
msgid "Create an IRModule"
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:50003
msgid ""
"IRModule can be created by writing TVMScript, which is a round-trippable "
"syntax for TVM IR."
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:50005
msgid ""
"Different than creating a computational expression by Tensor Expression "
"(`tutorial-tensor-expr-get-started`), TensorIR allow users to program "
"through TVMScript, a language embedded in python AST. The new method "
"makes it possible to write complex programs and further schedule and "
"optimize it."
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:50010
msgid "Following is a simple example for vector addition."
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:70002
msgid ""
"Besides, we can also use tensor expression DSL to write simple operators,"
" and convert them to an IRModule."
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:90002
msgid "Build and Run an IRModule"
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:90003
msgid ""
"We can build the IRModule into a runnable module with specific target "
"backends."
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:110002
msgid "Prepare the input array and output array, then run the module."
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:130002
msgid "Transform an IRModule"
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:130003
msgid ""
"The IRModule is the central data structure for program optimization, "
"which can be transformed by :code:`Schedule`. A schedule contains "
"multiple primitive methods to interactively transform the program. Each "
"primitive transforms the program in certain ways to bring additional "
"performance optimizations."
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:130010
msgid ""
"The image above is a typical workflow for optimizing a tensor program. "
"First, we need to create a schedule on the initial IRModule created from "
"either TVMScript or Tensor Expression. Then, a sequence of schedule "
"primitives will help to improve the performance. And at last, we can "
"lower and build it into a runnable module."
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:130015
msgid ""
"Here we just demostrate a very simple tranformation. First we create "
"schedule on the input `ir_module`."
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:150002
msgid "Tile the loop into 3 loops and print the result."
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:170002
msgid "We can also reorder the loops. Now we move loop `i_2` to outside of `i_1`."
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:190002
msgid "Transform to a GPU program"
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:190003
msgid ""
"If we want to deploy models on GPUs, thread binding is necessary. "
"Fortunately, we can also use primitives and do incrementally "
"transformation."
msgstr ""

#: ../../user-guide/tutorial/tensor_ir_blitz_course.ipynb:210002
msgid ""
"After binding the threads, now build the IRModule with :code:`cuda` "
"backends."
msgstr ""

