{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 在 VTA 上部署预训练的 Darknet 视觉检测模型\n",
        "\n",
        "**原作者**: [Hua Jiang](https://github.com/huajsj)\n",
        "\n",
        "本教程提供了端到端 demo，关于如何在 VTA 加速器设计上运行 Darknet YoloV3-tiny inference 来执行图像检测任务。它展示了 Relay 作为前端编译器，它可以执行量化（VTA 只支持 int8/32 推理）和 graph packing（为了在 core 中支持张量），从而为硬件目标 massage 计算图（compute graph）。\n",
        "\n",
        "## 安装依赖\n",
        "\n",
        "要在 `tvm` 中使用 `autotvm` 包，需要安装一些额外的依赖项。（如果你使用 python2，将 \"3\" 改为 \"2\"）：\n",
        "\n",
        "```bash\n",
        "pip3 install \"Pillow<7\"\n",
        "```\n",
        "\n",
        "带有 Darknet 解析的 YOLO-V3-tiny Model 依赖于 CFFI 和 CV2 库，需要在执行此脚本前安装 CFFI 和 CV2。\n",
        "\n",
        "```bash\n",
        "pip3 install cffi\n",
        "pip3 install opencv-python\n",
        "```\n",
        "\n",
        "现在返回 python 代码。导入包。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tvm\n",
        "import vta\n",
        "from tvm import rpc, autotvm, relay\n",
        "from tvm.relay.testing import yolo_detection, darknet\n",
        "from tvm.relay.testing.darknet import __darknetffi__\n",
        "from tvm.contrib import graph_executor, utils\n",
        "from tvm.contrib.download import download_testdata\n",
        "from vta.testing import simulator\n",
        "from vta.top import graph_pack\n",
        "\n",
        "# Make sure that TVM was compiled with RPC=1\n",
        "assert tvm.runtime.enabled(\"rpc\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "根据 Model 名称 下载 yolo net 配置文件，权重文件，darknet 库文件："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"yolov3-tiny\"\n",
        "REPO_URL = \"https://github.com/dmlc/web-data/blob/main/darknet/\"\n",
        "\n",
        "cfg_path = download_testdata(\n",
        "    \"https://github.com/pjreddie/darknet/blob/master/cfg/\" + MODEL_NAME + \".cfg\" + \"?raw=true\",\n",
        "    MODEL_NAME + \".cfg\",\n",
        "    module=\"darknet\",\n",
        ")\n",
        "weights_path = download_testdata(\n",
        "    \"https://pjreddie.com/media/files/\" + MODEL_NAME + \".weights\" + \"?raw=true\",\n",
        "    MODEL_NAME + \".weights\",\n",
        "    module=\"darknet\",\n",
        ")\n",
        "\n",
        "if sys.platform in [\"linux\", \"linux2\"]:\n",
        "    darknet_lib_path = download_testdata(\n",
        "        REPO_URL + \"lib/\" + \"libdarknet2.0.so\" + \"?raw=true\", \"libdarknet2.0.so\", module=\"darknet\"\n",
        "    )\n",
        "elif sys.platform == \"darwin\":\n",
        "    darknet_lib_path = download_testdata(\n",
        "        REPO_URL + \"lib_osx/\" + \"libdarknet_mac2.0.so\" + \"?raw=true\",\n",
        "        \"libdarknet_mac2.0.so\",\n",
        "        module=\"darknet\",\n",
        "    )\n",
        "else:\n",
        "    raise NotImplementedError(\"Darknet lib is not supported on {} platform\".format(sys.platform))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "下载 yolo 类别和 illustration 前端："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RemoteDisconnected('Remote end closed connection without response')\n",
            "Download attempt 0/3 failed, retrying.\n",
            "RemoteDisconnected('Remote end closed connection without response')\n",
            "Download attempt 0/3 failed, retrying.\n"
          ]
        }
      ],
      "source": [
        "coco_path = download_testdata(\n",
        "    REPO_URL + \"data/\" + \"coco.names\" + \"?raw=true\", \"coco.names\", module=\"data\"\n",
        ")\n",
        "font_path = download_testdata(\n",
        "    REPO_URL + \"data/\" + \"arial.ttf\" + \"?raw=true\", \"arial.ttf\", module=\"data\"\n",
        ")\n",
        "with open(coco_path) as f:\n",
        "    content = f.readlines()\n",
        "names = [x.strip() for x in content]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the platform and model targets.\n",
        "--------------------------------------\n",
        "Execute on CPU vs. VTA, and define the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load VTA parameters from the 3rdparty/vta-hw/config/vta_config.json file\n",
        "env = vta.get_env()\n",
        "# Set ``device=arm_cpu`` to run inference on the CPU\n",
        "# or ``device=vta`` to run inference on the FPGA.\n",
        "device = \"vta\"\n",
        "target = env.target if device == \"vta\" else env.target_vta_cpu\n",
        "\n",
        "pack_dict = {\n",
        "    \"yolov3-tiny\": [\"nn.max_pool2d\", \"cast\", 4, 186],\n",
        "}\n",
        "\n",
        "# Name of Darknet model to compile\n",
        "# The ``start_pack`` and ``stop_pack`` labels indicate where\n",
        "# to start and end the graph packing relay pass: in other words\n",
        "# where to start and finish offloading to VTA.\n",
        "# the number 4 indicate the the ``start_pack`` index is 4, the\n",
        "# number 186 indicate the ``stop_pack index`` is 186, by using\n",
        "# name and index number, here we can located to correct place\n",
        "# where to start/end when there are multiple ``nn.max_pool2d``\n",
        "# or ``cast``, print(mod.astext(show_meta_data=False)) can help\n",
        "# to find operator name and index information.\n",
        "assert MODEL_NAME in pack_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtain an execution remote.\n",
        "---------------------------\n",
        "When target is 'pynq' or other FPGA backend, reconfigure FPGA and runtime.\n",
        "Otherwise, if target is 'sim', execute locally.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if env.TARGET not in [\"sim\", \"tsim\"]:\n",
        "    # Get remote from tracker node if environment variable is set.\n",
        "    # To set up the tracker, you'll need to follow the \"Auto-tuning\n",
        "    # a convolutional network for VTA\" tutorial.\n",
        "    tracker_host = os.environ.get(\"TVM_TRACKER_HOST\", None)\n",
        "    tracker_port = os.environ.get(\"TVM_TRACKER_PORT\", None)\n",
        "    # Otherwise if you have a device you want to program directly from\n",
        "    # the host, make sure you've set the variables below to the IP of\n",
        "    # your board.\n",
        "    device_host = os.environ.get(\"VTA_RPC_HOST\", \"192.168.2.99\")\n",
        "    device_port = os.environ.get(\"VTA_RPC_PORT\", \"9091\")\n",
        "    if not tracker_host or not tracker_port:\n",
        "        remote = rpc.connect(device_host, int(device_port))\n",
        "    else:\n",
        "        remote = autotvm.measure.request_remote(\n",
        "            env.TARGET, tracker_host, int(tracker_port), timeout=10000\n",
        "        )\n",
        "    # Reconfigure the JIT runtime and FPGA.\n",
        "    # You can program the FPGA with your own custom bitstream\n",
        "    # by passing the path to the bitstream file instead of None.\n",
        "    reconfig_start = time.time()\n",
        "    vta.reconfig_runtime(remote)\n",
        "    vta.program_fpga(remote, bitstream=None)\n",
        "    reconfig_time = time.time() - reconfig_start\n",
        "    print(\"Reconfigured FPGA and RPC runtime in {0:.2f}s!\".format(reconfig_time))\n",
        "\n",
        "# In simulation mode, host the RPC server locally.\n",
        "else:\n",
        "    remote = rpc.LocalSession()\n",
        "\n",
        "# Get execution context from remote\n",
        "ctx = remote.ext_dev(0) if device == \"vta\" else remote.cpu(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build the inference graph executor.\n",
        "-----------------------------------\n",
        "Using Darknet library load downloaded vision model and compile with Relay.\n",
        "The compilation steps are:\n",
        "\n",
        "1. Front end translation from Darknet into Relay module.\n",
        "2. Apply 8-bit quantization: here we skip the first conv layer,\n",
        "   and dense layer which will both be executed in fp32 on the CPU.\n",
        "3. Perform graph packing to alter the data layout for tensorization.\n",
        "4. Perform constant folding to reduce number of operators (e.g. eliminate batch norm multiply).\n",
        "5. Perform relay build to object file.\n",
        "6. Load the object file onto remote (FPGA device).\n",
        "7. Generate graph executor, `m`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load pre-configured AutoTVM schedules\n",
        "with autotvm.tophub.context(target):\n",
        "    net = __darknetffi__.dlopen(darknet_lib_path).load_network(\n",
        "        cfg_path.encode(\"utf-8\"), weights_path.encode(\"utf-8\"), 0\n",
        "    )\n",
        "    dshape = (env.BATCH, net.c, net.h, net.w)\n",
        "    dtype = \"float32\"\n",
        "\n",
        "    # Measure build start time\n",
        "    build_start = time.time()\n",
        "\n",
        "    # Start front end compilation\n",
        "    mod, params = relay.frontend.from_darknet(net, dtype=dtype, shape=dshape)\n",
        "\n",
        "    if target.device_name == \"vta\":\n",
        "        # Perform quantization in Relay\n",
        "        # Note: We set opt_level to 3 in order to fold batch norm\n",
        "        with tvm.transform.PassContext(opt_level=3):\n",
        "            with relay.quantize.qconfig(\n",
        "                global_scale=23.0,\n",
        "                skip_conv_layers=[0],\n",
        "                store_lowbit_output=True,\n",
        "                round_for_shift=True,\n",
        "            ):\n",
        "                mod = relay.quantize.quantize(mod, params=params)\n",
        "            # Perform graph packing and constant folding for VTA target\n",
        "            mod = graph_pack(\n",
        "                mod[\"main\"],\n",
        "                env.BATCH,\n",
        "                env.BLOCK_OUT,\n",
        "                env.WGT_WIDTH,\n",
        "                start_name=pack_dict[MODEL_NAME][0],\n",
        "                stop_name=pack_dict[MODEL_NAME][1],\n",
        "                start_name_idx=pack_dict[MODEL_NAME][2],\n",
        "                stop_name_idx=pack_dict[MODEL_NAME][3],\n",
        "            )\n",
        "    else:\n",
        "        mod = mod[\"main\"]\n",
        "\n",
        "    # Compile Relay program with AlterOpLayout disabled\n",
        "    with vta.build_config(disabled_pass={\"AlterOpLayout\", \"tir.CommonSubexprElimTIR\"}):\n",
        "        lib = relay.build(\n",
        "            mod, target=tvm.target.Target(target, host=env.target_host), params=params\n",
        "        )\n",
        "\n",
        "    # Measure Relay build time\n",
        "    build_time = time.time() - build_start\n",
        "    print(MODEL_NAME + \" inference graph built in {0:.2f}s!\".format(build_time))\n",
        "\n",
        "    # Send the inference library over to the remote RPC server\n",
        "    temp = utils.tempdir()\n",
        "    lib.export_library(temp.relpath(\"graphlib.tar\"))\n",
        "    remote.upload(temp.relpath(\"graphlib.tar\"))\n",
        "    lib = remote.load_module(\"graphlib.tar\")\n",
        "\n",
        "    # Graph executor\n",
        "    m = graph_executor.GraphModule(lib[\"default\"](ctx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perform image detection inference.\n",
        "----------------------------------\n",
        "We run detect on an downloaded image\n",
        "Download test image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "[neth, netw] = dshape[2:]\n",
        "test_image = \"person.jpg\"\n",
        "img_url = REPO_URL + \"data/\" + test_image + \"?raw=true\"\n",
        "img_path = download_testdata(img_url, test_image, \"data\")\n",
        "data = darknet.load_image(img_path, neth, netw).transpose(1, 2, 0)\n",
        "\n",
        "# Prepare test image for inference\n",
        "plt.imshow(data)\n",
        "plt.show()\n",
        "data = data.transpose((2, 0, 1))\n",
        "data = data[np.newaxis, :]\n",
        "data = np.repeat(data, env.BATCH, axis=0)\n",
        "\n",
        "# Set the network parameters and inputs\n",
        "m.set_input(\"data\", data)\n",
        "\n",
        "# Perform inference and gather execution statistics\n",
        "# More on: :py:method:`tvm.runtime.Module.time_evaluator`\n",
        "num = 4  # number of times we run module for a single measurement\n",
        "rep = 3  # number of measurements (we derive std dev from this)\n",
        "timer = m.module.time_evaluator(\"run\", ctx, number=num, repeat=rep)\n",
        "\n",
        "if env.TARGET in [\"sim\", \"tsim\"]:\n",
        "    simulator.clear_stats()\n",
        "    timer()\n",
        "    sim_stats = simulator.stats()\n",
        "    print(\"\\nExecution statistics:\")\n",
        "    for k, v in sim_stats.items():\n",
        "        # Since we execute the workload many times, we need to normalize stats\n",
        "        # Note that there is always one warm up run\n",
        "        # Therefore we divide the overall stats by (num * rep + 1)\n",
        "        print(\"\\t{:<16}: {:>16}\".format(k, v // (num * rep + 1)))\n",
        "else:\n",
        "    tcost = timer()\n",
        "    std = np.std(tcost.results) * 1000\n",
        "    mean = tcost.mean * 1000\n",
        "    print(\"\\nPerformed inference in %.2fms (std = %.2f) for %d samples\" % (mean, std, env.BATCH))\n",
        "    print(\"Average per sample inference time: %.2fms\" % (mean / env.BATCH))\n",
        "\n",
        "# Get detection results from out\n",
        "thresh = 0.5\n",
        "nms_thresh = 0.45\n",
        "tvm_out = []\n",
        "for i in range(2):\n",
        "    layer_out = {}\n",
        "    layer_out[\"type\"] = \"Yolo\"\n",
        "    # Get the yolo layer attributes (n, out_c, out_h, out_w, classes, total)\n",
        "    layer_attr = m.get_output(i * 4 + 3).numpy()\n",
        "    layer_out[\"biases\"] = m.get_output(i * 4 + 2).numpy()\n",
        "    layer_out[\"mask\"] = m.get_output(i * 4 + 1).numpy()\n",
        "    out_shape = (layer_attr[0], layer_attr[1] // layer_attr[0], layer_attr[2], layer_attr[3])\n",
        "    layer_out[\"output\"] = m.get_output(i * 4).numpy().reshape(out_shape)\n",
        "    layer_out[\"classes\"] = layer_attr[4]\n",
        "    tvm_out.append(layer_out)\n",
        "    thresh = 0.560\n",
        "\n",
        "# Show detection results\n",
        "img = darknet.load_image_color(img_path)\n",
        "_, im_h, im_w = img.shape\n",
        "dets = yolo_detection.fill_network_boxes((netw, neth), (im_w, im_h), thresh, 1, tvm_out)\n",
        "last_layer = net.layers[net.n - 1]\n",
        "yolo_detection.do_nms_sort(dets, last_layer.classes, nms_thresh)\n",
        "yolo_detection.draw_detections(font_path, img, dets, thresh, names, last_layer.classes)\n",
        "plt.imshow(img.transpose(1, 2, 0))\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "f0a0fcc4cb7375f8ee907b3c51d5b9d65107fda1aab037a85df7b0c09b870b98"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 (conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
