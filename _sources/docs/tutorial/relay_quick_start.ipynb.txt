{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(tutorial-relay-quick-start)=\n",
        "\n",
        "# 编译深度学习模型的快速入门教程\n",
        "**作者**: [Yao Wang](https://github.com/kevinthesun), [Truman Tian](https://github.com/SiNZeRo)\n",
        "\n",
        "这个例子展示了如何用 Relay python 前端构建一个神经网络，并通过 TVM 为 Nvidia GPU 生成运行时库。注意，你需要在启用 cuda 和 llvm 的情况下构建 TVM。\n",
        "\n",
        "## 支持的 TVM 硬件后端概述\n",
        "\n",
        "下图显示了 TVM 目前支持的硬件后端：\n",
        "\n",
        "![](images/tvm_support_list.png)\n",
        "\n",
        "在本教程中，我们将选择 cuda 和 llvm 作为目标后端。首先，让我们导入 Relay 和 TVM。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from env import tvm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from tvm import relay\n",
        "from tvm.relay import testing\n",
        "import tvm\n",
        "from tvm import te\n",
        "from tvm.contrib import graph_executor\n",
        "import tvm.testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 在 Relay 中定义神经网络\n",
        "\n",
        "首先，用 relay 的 python 前端定义神经网络。为了简单起见，将使用 Relay 中预先定义的 resnet-18 网络。参数用 Xavier 初始化器进行初始化。Relay 也支持其他模型格式，如 MXNet、CoreML、ONNX 和 Tensorflow。\n",
        "\n",
        "在本教程中，假设将在我们的设备上进行推理，并且批量大小被设置为 1。输入图像是大小为 224*224 的 RGB 彩色图像。可以调用 {py:meth}`tvm.relay.expr.TupleWrapper.astext` 来显示网络结构。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#[version = \"0.0.5\"]\n",
            "def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {\n",
            "  %0 = nn.batch_norm(%data, %bn_data_gamma, %bn_data_beta, %bn_data_moving_mean, %bn_data_moving_var, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;\n",
            "  %1 = %0.0;\n",
            "  %2 = nn.conv2d(%1, %conv0_weight, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;\n",
            "  %3 = nn.batch_norm(%2, %bn0_gamma, %bn0_beta, %bn0_moving_mean, %bn0_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
            "  %4 = %3.0;\n",
            "  %5 = nn.relu(%4) /* ty=Tensor[(1, 64, 112, 112), float32] */;\n",
            "  %6 = nn.max_pool2d(%5, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
            "  %7 = nn.batch_norm(%6, %stage1_unit1_bn1_gamma, %stage1_unit1_bn1_beta, %stage1_unit1_bn1_moving_mean, %stage1_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
            "  %8 = %7.0;\n",
            "  %9 = nn.relu(%8) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
            "  %10 = nn.conv2d(%9, %stage1_unit1_conv1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
            "  %11 = nn.batch_norm(%10, %stage1_unit1_bn2_gamma, %stage1_unit1_bn2_beta, %stage1_unit1_bn2_moving_mean, %stage1_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
            "  %12 = %11.0;\n",
            "  %13 = nn.relu(%12) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
            "  %14 = nn.conv2d(%13, %stage1_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
            "  %15 = nn.conv2d(%9, %stage1_unit1_sc_weight, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
            "  %16 = add(%14, %15) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
            "  %17 = nn.batch_norm(%16, %stage1_unit2_bn1_gamma, %stage1_unit2_bn1_beta, %stage1_unit2_bn1_moving_mean, %stage1_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
            "  %18 = %17.0;\n",
            "  %19 = nn.relu(%18) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
            "  %20 = nn.conv2d(%19, %stage1_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
            "  %21 = nn.batch_norm(%20, %stage1_unit2_bn2_gamma, %stage1_unit2_bn2_beta, %stage1_unit2_bn2_moving_mean, %stage1_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
            "  %22 = %21.0;\n",
            "  %23 = nn.relu(%22) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
            "  %24 = nn.conv2d(%23, %stage1_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
            "  %25 = add(%24, %16) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
            "  %26 = nn.batch_norm(%25, %stage2_unit1_bn1_gamma, %stage2_unit1_bn1_beta, %stage2_unit1_bn1_moving_mean, %stage2_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;\n",
            "  %27 = %26.0;\n",
            "  %28 = nn.relu(%27) /* ty=Tensor[(1, 64, 56, 56), float32] */;\n",
            "  %29 = nn.conv2d(%28, %stage2_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
            "  %30 = nn.batch_norm(%29, %stage2_unit1_bn2_gamma, %stage2_unit1_bn2_beta, %stage2_unit1_bn2_moving_mean, %stage2_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;\n",
            "  %31 = %30.0;\n",
            "  %32 = nn.relu(%31) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
            "  %33 = nn.conv2d(%32, %stage2_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
            "  %34 = nn.conv2d(%28, %stage2_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
            "  %35 = add(%33, %34) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
            "  %36 = nn.batch_norm(%35, %stage2_unit2_bn1_gamma, %stage2_unit2_bn1_beta, %stage2_unit2_bn1_moving_mean, %stage2_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;\n",
            "  %37 = %36.0;\n",
            "  %38 = nn.relu(%37) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
            "  %39 = nn.conv2d(%38, %stage2_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
            "  %40 = nn.batch_norm(%39, %stage2_unit2_bn2_gamma, %stage2_unit2_bn2_beta, %stage2_unit2_bn2_moving_mean, %stage2_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;\n",
            "  %41 = %40.0;\n",
            "  %42 = nn.relu(%41) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
            "  %43 = nn.conv2d(%42, %stage2_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
            "  %44 = add(%43, %35) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
            "  %45 = nn.batch_norm(%44, %stage3_unit1_bn1_gamma, %stage3_unit1_bn1_beta, %stage3_unit1_bn1_moving_mean, %stage3_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;\n",
            "  %46 = %45.0;\n",
            "  %47 = nn.relu(%46) /* ty=Tensor[(1, 128, 28, 28), float32] */;\n",
            "  %48 = nn.conv2d(%47, %stage3_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
            "  %49 = nn.batch_norm(%48, %stage3_unit1_bn2_gamma, %stage3_unit1_bn2_beta, %stage3_unit1_bn2_moving_mean, %stage3_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
            "  %50 = %49.0;\n",
            "  %51 = nn.relu(%50) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
            "  %52 = nn.conv2d(%51, %stage3_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
            "  %53 = nn.conv2d(%47, %stage3_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
            "  %54 = add(%52, %53) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
            "  %55 = nn.batch_norm(%54, %stage3_unit2_bn1_gamma, %stage3_unit2_bn1_beta, %stage3_unit2_bn1_moving_mean, %stage3_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
            "  %56 = %55.0;\n",
            "  %57 = nn.relu(%56) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
            "  %58 = nn.conv2d(%57, %stage3_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
            "  %59 = nn.batch_norm(%58, %stage3_unit2_bn2_gamma, %stage3_unit2_bn2_beta, %stage3_unit2_bn2_moving_mean, %stage3_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
            "  %60 = %59.0;\n",
            "  %61 = nn.relu(%60) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
            "  %62 = nn.conv2d(%61, %stage3_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
            "  %63 = add(%62, %54) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
            "  %64 = nn.batch_norm(%63, %stage4_unit1_bn1_gamma, %stage4_unit1_bn1_beta, %stage4_unit1_bn1_moving_mean, %stage4_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;\n",
            "  %65 = %64.0;\n",
            "  %66 = nn.relu(%65) /* ty=Tensor[(1, 256, 14, 14), float32] */;\n",
            "  %67 = nn.conv2d(%66, %stage4_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
            "  %68 = nn.batch_norm(%67, %stage4_unit1_bn2_gamma, %stage4_unit1_bn2_beta, %stage4_unit1_bn2_moving_mean, %stage4_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;\n",
            "  %69 = %68.0;\n",
            "  %70 = nn.relu(%69) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
            "  %71 = nn.conv2d(%70, %stage4_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
            "  %72 = nn.conv2d(%66, %stage4_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
            "  %73 = add(%71, %72) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
            "  %74 = nn.batch_norm(%73, %stage4_unit2_bn1_gamma, %stage4_unit2_bn1_beta, %stage4_unit2_bn1_moving_mean, %stage4_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;\n",
            "  %75 = %74.0;\n",
            "  %76 = nn.relu(%75) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
            "  %77 = nn.conv2d(%76, %stage4_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
            "  %78 = nn.batch_norm(%77, %stage4_unit2_bn2_gamma, %stage4_unit2_bn2_beta, %stage4_unit2_bn2_moving_mean, %stage4_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;\n",
            "  %79 = %78.0;\n",
            "  %80 = nn.relu(%79) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
            "  %81 = nn.conv2d(%80, %stage4_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
            "  %82 = add(%81, %73) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
            "  %83 = nn.batch_norm(%82, %bn1_gamma, %bn1_beta, %bn1_moving_mean, %bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;\n",
            "  %84 = %83.0;\n",
            "  %85 = nn.relu(%84) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
            "  %86 = nn.global_avg_pool2d(%85) /* ty=Tensor[(1, 512, 1, 1), float32] */;\n",
            "  %87 = nn.batch_flatten(%86) /* ty=Tensor[(1, 512), float32] */;\n",
            "  %88 = nn.dense(%87, %fc1_weight, units=1000) /* ty=Tensor[(1, 1000), float32] */;\n",
            "  %89 = nn.bias_add(%88, %fc1_bias, axis=-1) /* ty=Tensor[(1, 1000), float32] */;\n",
            "  nn.softmax(%89) /* ty=Tensor[(1, 1000), float32] */\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "batch_size = 1\n",
        "num_class = 1000\n",
        "image_shape = (3, 224, 224)\n",
        "data_shape = (batch_size,) + image_shape\n",
        "out_shape = (batch_size, num_class)\n",
        "\n",
        "mod, params = relay.testing.resnet.get_workload(\n",
        "    num_layers=18, batch_size=batch_size, image_shape=image_shape\n",
        ")\n",
        "\n",
        "# set show_meta_data=True if you want to show meta data\n",
        "print(mod.astext(show_meta_data=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 编译\n",
        "\n",
        "下一步是使用 Relay/TVM 管道对模型进行编译。用户可以指定编译的优化级别。目前这个值可以是 0 到 3。优化 passes 包括算子融合（operator fusion）、预计算（pre-computation）、布局变换（layout transformation）等。\n",
        "\n",
        "{py:func}`relay.build` 返回三个部分：json 格式的执行图，TVM 模块库中专门为这个图在目标硬件上编译的函数，以及模型的参数 blobs。在编译过程中，Relay 做了图层面的优化，而 TVM 做了张量层面的优化，从而产生了一个优化的运行模块为模型服务。\n",
        "\n",
        "我们将首先为 Nvidia GPU 进行编译。在幕后， {py:func}`relay.build` 首先做了一些图层面的优化，例如修剪（pruning）、融合（fusing）等，然后将运算符（即优化后的图的节点）注册到 TVM 实现中，生成 `tvm.module`。为了生成模块库，TVM 将首先把高层 IR 转移到指定目标后端的低层内在 IR 中，在这个例子中是 CUDA。然后机器代码将被生成为模块库。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/target/target.py:347: UserWarning: Try specifying cuda arch by adding 'arch=sm_xx' to your target.\n",
            "  warnings.warn(\"Try specifying cuda arch by adding 'arch=sm_xx' to your target.\")\n",
            "[00:14:27] /home/xinet/study/tvm/src/target/target_kind.cc:163: Warning: Unable to detect CUDA version, default to \"-arch=sm_20\" instead\n",
            "URLError(ConnectionRefusedError(111, 'Connection refused'))\n",
            "Download attempt 0/3 failed, retrying.\n",
            "URLError(ConnectionRefusedError(111, 'Connection refused'))\n",
            "Download attempt 1/3 failed, retrying.\n",
            "WARNING:root:Failed to download tophub package for cuda: <urlopen error [Errno 111] Connection refused>\n",
            "WARNING:autotvm:One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n",
            "WARNING:download:URLError(ConnectionRefusedError(111, 'Connection refused'))\n",
            "Download attempt 0/3 failed, retrying.\n",
            "WARNING:download:URLError(ConnectionRefusedError(111, 'Connection refused'))\n",
            "Download attempt 1/3 failed, retrying.\n"
          ]
        },
        {
          "ename": "TVMError",
          "evalue": "Traceback (most recent call last):\n  133: TVMFuncCall\n  132: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  131: tvm::relay::backend::RelayBuildModule::Build(tvm::IRModule, tvm::runtime::Array<tvm::Target, void> const&, tvm::relay::Executor const&, tvm::relay::Runtime const&, tvm::WorkspaceMemoryPools const&, tvm::runtime::String const&)\n  130: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)\n  129: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::relay::backend::GraphExecutorCodegenModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  128: tvm::relay::backend::GraphExecutorCodegen::Codegen(tvm::IRModule, tvm::relay::Function, tvm::runtime::String)\n  127: tvm::transform::Pass::operator()(tvm::IRModule) const\n  126: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  125: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  124: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  123: tvm::transform::ModulePassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  122: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_8IRModuleES5_NS_9transform11PassContextEEE17AssignTypedLambdaIZNS_5relay3tec11LowerTEPassERKNS0_6StringESt8functionIFvNS_8BaseFuncEEENS_13VirtualDeviceEEUlS5_S7_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SN_SR_\n  121: tvm::relay::tec::LowerTE(tvm::IRModule const&, tvm::runtime::String const&, std::function<void (tvm::BaseFunc)>, tvm::VirtualDevice)\n  120: tvm::transform::Pass::operator()(tvm::IRModule) const\n  119: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  118: tvm::relay::transform::FunctionPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  117: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_5relay8FunctionES6_NS_8IRModuleENS_9transform11PassContextEEE17AssignTypedLambdaIZNS5_3tec15LowerTensorExprERKNS0_6StringENSD_10TECompilerESt8functionIFvNS_8BaseFuncEEENS_13VirtualDeviceEEUlS6_S7_S9_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SP_ST_\n  116: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  115: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  114: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::FunctionNode const*)\n  113: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::FunctionNode const*)\n  112: _ZN3tvm5relay9transform22DeviceAwareExprMutator21DeviceAwareVisit\n  111: tvm::relay::ExprMutator::VisitExpr_(tvm::relay::FunctionNode const*)\n  110: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  109: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  108: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  107: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  106: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  105: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  104: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  103: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  102: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  101: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  100: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  99: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  98: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  97: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  96: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  95: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  94: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  93: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  92: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  91: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  90: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  89: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  88: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  87: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  86: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  85: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  84: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  83: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  82: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  81: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  80: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  79: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  78: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  77: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  76: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  75: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  74: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  73: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  72: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  71: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  70: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  69: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  68: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  67: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  66: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  65: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  64: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  63: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  62: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  61: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  60: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  59: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  58: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  57: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  56: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  55: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  54: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  53: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  52: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  51: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  50: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  49: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  48: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  47: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  46: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  45: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  44: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  43: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  42: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  41: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  40: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  39: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  38: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  37: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  36: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  35: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  34: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  33: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  32: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  31: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  30: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  29: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  28: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  27: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  26: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  25: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  24: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  23: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  22: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  21: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  20: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  19: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  18: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  17: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  16: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  15: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  14: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  13: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  12: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  11: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  10: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  9: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  8: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  7: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  6: tvm::relay::tec::LowerTensorExprMutator::MakeLoweredCall(tvm::relay::Function, tvm::runtime::Array<tvm::RelayExpr, void>, tvm::Span, tvm::Target)\n  5: tvm::relay::tec::TECompilerImpl::Lower(tvm::relay::tec::CCacheKey const&, tvm::runtime::String)\n  4: tvm::relay::tec::TECompilerImpl::LowerInternal(tvm::relay::tec::CCacheKey const&, std::function<tvm::runtime::String (tvm::runtime::String)>)\n  3: tvm::relay::tec::PrimFuncFor(tvm::relay::Function const&, tvm::Target const&, std::function<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > (std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)>)\n  2: tvm::relay::tec::ScheduleBuilder::Create(tvm::relay::Function const&, std::function<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > (std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)>)\n  1: tvm::relay::OpImplementation::Schedule(tvm::Attrs const&, tvm::runtime::Array<tvm::te::Tensor, void> const&, tvm::Target const&)\n  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .cold]\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/urllib/request.py\", line 1348, in do_open\n    h.request(req.get_method(), req.selector, req.data, headers,\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/http/client.py\", line 1282, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/http/client.py\", line 1328, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/http/client.py\", line 1277, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/http/client.py\", line 1037, in _send_output\n    self.send(msg)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/http/client.py\", line 975, in send\n    self.connect()\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/http/client.py\", line 1447, in connect\n    super().connect()\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/http/client.py\", line 941, in connect\n    self.sock = self._create_connection(\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/socket.py\", line 845, in create_connection\n    raise err\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/socket.py\", line 833, in create_connection\n    sock.connect(sa)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/_ffi/_ctypes/packed_func.py\", line 81, in cfun\n    rv = local_pyfunc(*pyargs)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/relay/op/strategy/generic.py\", line 51, in wrapper\n    return topi_schedule(outs)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/autotvm/task/topi_integration.py\", line 242, in wrapper\n    return topi_schedule(cfg, outs, *args, **kwargs)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/topi/cuda/conv2d.py\", line 46, in schedule_conv2d_nchw\n    traverse_inline(s, outs[0].op, _callback)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/topi/utils.py\", line 81, in traverse_inline\n    _traverse(final_op)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/topi/utils.py\", line 78, in _traverse\n    _traverse(tensor.op)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/topi/utils.py\", line 78, in _traverse\n    _traverse(tensor.op)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/topi/utils.py\", line 79, in _traverse\n    callback(op)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/topi/cuda/conv2d.py\", line 44, in _callback\n    schedule_direct_cuda(cfg, s, op.output(0))\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/topi/cuda/conv2d_direct.py\", line 47, in schedule_direct_cuda\n    ref_log = autotvm.tophub.load_reference_log(\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/autotvm/tophub.py\", line 220, in load_reference_log\n    download_package(tophub_location, package_name)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/autotvm/tophub.py\", line 182, in download_package\n    download(download_url, Path(rootpath, package_name), overwrite=True)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/contrib/download.py\", line 125, in download\n    raise err\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/contrib/download.py\", line 112, in download\n    urllib2.urlretrieve(url, download_loc, reporthook=_download_progress)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/urllib/request.py\", line 241, in urlretrieve\n    with contextlib.closing(urlopen(url, data)) as fp:\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/urllib/request.py\", line 216, in urlopen\n    return opener.open(url, data, timeout)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/urllib/request.py\", line 519, in open\n    response = self._open(req, data)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/urllib/request.py\", line 536, in _open\n    result = self._call_chain(self.handle_open, protocol, protocol +\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n    result = func(*args)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/urllib/request.py\", line 1391, in https_open\n    return self.do_open(http.client.HTTPSConnection, req,\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/urllib/request.py\", line 1351, in do_open\n    raise URLError(err)\nConnectionRefusedError: [Errno 111] Connection refused\nDuring handling of the above exception, another exception occurred:\n\nurllib.error.URLError: <urlopen error [Errno 111] Connection refused>",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32m/home/xinet/study/tvm/xinetzone/docs/tutorial/relay_quick_start.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xinet/study/tvm/xinetzone/docs/tutorial/relay_quick_start.ipynb#ch0000006vscode-remote?line=1'>2</a>\u001b[0m target \u001b[39m=\u001b[39m tvm\u001b[39m.\u001b[39mtarget\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xinet/study/tvm/xinetzone/docs/tutorial/relay_quick_start.ipynb#ch0000006vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m tvm\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mPassContext(opt_level\u001b[39m=\u001b[39mopt_level):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/xinet/study/tvm/xinetzone/docs/tutorial/relay_quick_start.ipynb#ch0000006vscode-remote?line=3'>4</a>\u001b[0m     lib \u001b[39m=\u001b[39m relay\u001b[39m.\u001b[39;49mbuild(mod, target, params\u001b[39m=\u001b[39;49mparams)\n",
            "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/relay/build_module.py:470\u001b[0m, in \u001b[0;36mbuild\u001b[0;34m(ir_mod, target, target_host, executor, runtime, workspace_memory_pools, params, mod_name)\u001b[0m\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/relay/build_module.py?line=467'>468</a>\u001b[0m \u001b[39mwith\u001b[39;00m tophub_context:\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/relay/build_module.py?line=468'>469</a>\u001b[0m     bld_mod \u001b[39m=\u001b[39m BuildModule()\n\u001b[0;32m--> <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/relay/build_module.py?line=469'>470</a>\u001b[0m     graph_json, runtime_mod, params \u001b[39m=\u001b[39m bld_mod\u001b[39m.\u001b[39;49mbuild(\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/relay/build_module.py?line=470'>471</a>\u001b[0m         mod\u001b[39m=\u001b[39;49mir_mod,\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/relay/build_module.py?line=471'>472</a>\u001b[0m         target\u001b[39m=\u001b[39;49mtarget,\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/relay/build_module.py?line=472'>473</a>\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/relay/build_module.py?line=473'>474</a>\u001b[0m         executor\u001b[39m=\u001b[39;49mexecutor,\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/relay/build_module.py?line=474'>475</a>\u001b[0m         runtime\u001b[39m=\u001b[39;49mruntime,\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/relay/build_module.py?line=475'>476</a>\u001b[0m         workspace_memory_pools\u001b[39m=\u001b[39;49mworkspace_memory_pools,\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/relay/build_module.py?line=476'>477</a>\u001b[0m         mod_name\u001b[39m=\u001b[39;49mmod_name,\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/relay/build_module.py?line=477'>478</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/relay/build_module.py?line=478'>479</a>\u001b[0m     func_metadata \u001b[39m=\u001b[39m bld_mod\u001b[39m.\u001b[39mget_function_metadata()\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/relay/build_module.py?line=479'>480</a>\u001b[0m     devices \u001b[39m=\u001b[39m bld_mod\u001b[39m.\u001b[39mget_devices()\n",
            "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/relay/build_module.py:194\u001b[0m, in \u001b[0;36mBuildModule.build\u001b[0;34m(self, mod, target, target_host, executor, runtime, workspace_memory_pools, params, mod_name)\u001b[0m\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/relay/build_module.py?line=189'>190</a>\u001b[0m autotvm\u001b[39m.\u001b[39mGLOBAL_SCOPE\u001b[39m.\u001b[39msilent \u001b[39m=\u001b[39m use_auto_scheduler \u001b[39mor\u001b[39;00m old_autotvm_silent\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/relay/build_module.py?line=191'>192</a>\u001b[0m mod_name \u001b[39m=\u001b[39m mangle_module_name(mod_name)\n\u001b[0;32m--> <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/relay/build_module.py?line=193'>194</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build(mod, raw_targets, executor, runtime, workspace_memory_pools, mod_name)\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/relay/build_module.py?line=194'>195</a>\u001b[0m autotvm\u001b[39m.\u001b[39mGLOBAL_SCOPE\u001b[39m.\u001b[39msilent \u001b[39m=\u001b[39m old_autotvm_silent\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/relay/build_module.py?line=196'>197</a>\u001b[0m \u001b[39m# Get artifacts\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/_ffi/_ctypes/packed_func.py:237\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/_ffi/_ctypes/packed_func.py?line=224'>225</a>\u001b[0m ret_tcode \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/_ffi/_ctypes/packed_func.py?line=225'>226</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/_ffi/_ctypes/packed_func.py?line=226'>227</a>\u001b[0m     _LIB\u001b[39m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/_ffi/_ctypes/packed_func.py?line=227'>228</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/_ffi/_ctypes/packed_func.py?line=234'>235</a>\u001b[0m     \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/_ffi/_ctypes/packed_func.py?line=235'>236</a>\u001b[0m ):\n\u001b[0;32m--> <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/_ffi/_ctypes/packed_func.py?line=236'>237</a>\u001b[0m     \u001b[39mraise\u001b[39;00m get_last_ffi_error()\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/_ffi/_ctypes/packed_func.py?line=237'>238</a>\u001b[0m _ \u001b[39m=\u001b[39m temp_args\n\u001b[1;32m    <a href='file:///home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/_ffi/_ctypes/packed_func.py?line=238'>239</a>\u001b[0m _ \u001b[39m=\u001b[39m args\n",
            "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  133: TVMFuncCall\n  132: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::relay::backend::RelayBuildModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  131: tvm::relay::backend::RelayBuildModule::Build(tvm::IRModule, tvm::runtime::Array<tvm::Target, void> const&, tvm::relay::Executor const&, tvm::relay::Runtime const&, tvm::WorkspaceMemoryPools const&, tvm::runtime::String const&)\n  130: tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::IRModule, tvm::runtime::String const&)\n  129: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::relay::backend::GraphExecutorCodegenModule::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  128: tvm::relay::backend::GraphExecutorCodegen::Codegen(tvm::IRModule, tvm::relay::Function, tvm::runtime::String)\n  127: tvm::transform::Pass::operator()(tvm::IRModule) const\n  126: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  125: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  124: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  123: tvm::transform::ModulePassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  122: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_8IRModuleES5_NS_9transform11PassContextEEE17AssignTypedLambdaIZNS_5relay3tec11LowerTEPassERKNS0_6StringESt8functionIFvNS_8BaseFuncEEENS_13VirtualDeviceEEUlS5_S7_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SN_SR_\n  121: tvm::relay::tec::LowerTE(tvm::IRModule const&, tvm::runtime::String const&, std::function<void (tvm::BaseFunc)>, tvm::VirtualDevice)\n  120: tvm::transform::Pass::operator()(tvm::IRModule) const\n  119: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  118: tvm::relay::transform::FunctionPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  117: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_5relay8FunctionES6_NS_8IRModuleENS_9transform11PassContextEEE17AssignTypedLambdaIZNS5_3tec15LowerTensorExprERKNS0_6StringENSD_10TECompilerESt8functionIFvNS_8BaseFuncEEENS_13VirtualDeviceEEUlS6_S7_S9_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SP_ST_\n  116: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  115: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  114: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::FunctionNode const*)\n  113: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::FunctionNode const*)\n  112: _ZN3tvm5relay9transform22DeviceAwareExprMutator21DeviceAwareVisit\n  111: tvm::relay::ExprMutator::VisitExpr_(tvm::relay::FunctionNode const*)\n  110: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  109: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  108: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  107: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  106: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  105: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  104: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  103: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  102: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  101: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  100: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  99: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  98: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  97: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  96: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  95: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  94: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  93: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  92: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  91: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  90: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  89: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  88: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  87: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  86: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  85: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  84: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  83: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  82: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  81: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  80: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  79: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  78: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  77: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  76: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  75: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  74: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  73: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  72: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  71: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  70: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  69: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  68: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  67: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  66: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  65: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  64: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  63: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  62: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  61: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  60: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  59: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  58: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  57: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  56: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  55: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  54: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  53: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  52: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  51: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  50: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  49: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  48: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  47: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  46: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  45: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  44: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  43: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  42: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  41: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  40: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  39: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  38: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  37: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  36: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  35: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  34: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  33: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  32: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  31: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  30: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  29: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  28: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  27: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  26: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  25: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  24: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  23: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  22: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  21: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  20: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  19: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  18: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  17: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  16: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  15: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  14: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  13: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  12: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  11: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  10: tvm::relay::ExprMutator::VisitExpr(tvm::RelayExpr const&)\n  9: _ZZN3tvm5relay11ExprFunctorIFNS_9RelayExprERKS2_EE10InitVTableEvENUlRKNS_7r\n  8: tvm::relay::transform::DeviceAwareExprMutator::VisitExpr_(tvm::relay::CallNode const*)\n  7: tvm::relay::tec::LowerTensorExprMutator::DeviceAwareVisitExpr_(tvm::relay::CallNode const*)\n  6: tvm::relay::tec::LowerTensorExprMutator::MakeLoweredCall(tvm::relay::Function, tvm::runtime::Array<tvm::RelayExpr, void>, tvm::Span, tvm::Target)\n  5: tvm::relay::tec::TECompilerImpl::Lower(tvm::relay::tec::CCacheKey const&, tvm::runtime::String)\n  4: tvm::relay::tec::TECompilerImpl::LowerInternal(tvm::relay::tec::CCacheKey const&, std::function<tvm::runtime::String (tvm::runtime::String)>)\n  3: tvm::relay::tec::PrimFuncFor(tvm::relay::Function const&, tvm::Target const&, std::function<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > (std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)>)\n  2: tvm::relay::tec::ScheduleBuilder::Create(tvm::relay::Function const&, std::function<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > (std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)>)\n  1: tvm::relay::OpImplementation::Schedule(tvm::Attrs const&, tvm::runtime::Array<tvm::te::Tensor, void> const&, tvm::Target const&)\n  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .cold]\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/urllib/request.py\", line 1348, in do_open\n    h.request(req.get_method(), req.selector, req.data, headers,\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/http/client.py\", line 1282, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/http/client.py\", line 1328, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/http/client.py\", line 1277, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/http/client.py\", line 1037, in _send_output\n    self.send(msg)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/http/client.py\", line 975, in send\n    self.connect()\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/http/client.py\", line 1447, in connect\n    super().connect()\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/http/client.py\", line 941, in connect\n    self.sock = self._create_connection(\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/socket.py\", line 845, in create_connection\n    raise err\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/socket.py\", line 833, in create_connection\n    sock.connect(sa)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/_ffi/_ctypes/packed_func.py\", line 81, in cfun\n    rv = local_pyfunc(*pyargs)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/relay/op/strategy/generic.py\", line 51, in wrapper\n    return topi_schedule(outs)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/autotvm/task/topi_integration.py\", line 242, in wrapper\n    return topi_schedule(cfg, outs, *args, **kwargs)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/topi/cuda/conv2d.py\", line 46, in schedule_conv2d_nchw\n    traverse_inline(s, outs[0].op, _callback)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/topi/utils.py\", line 81, in traverse_inline\n    _traverse(final_op)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/topi/utils.py\", line 78, in _traverse\n    _traverse(tensor.op)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/topi/utils.py\", line 78, in _traverse\n    _traverse(tensor.op)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/topi/utils.py\", line 79, in _traverse\n    callback(op)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/topi/cuda/conv2d.py\", line 44, in _callback\n    schedule_direct_cuda(cfg, s, op.output(0))\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/topi/cuda/conv2d_direct.py\", line 47, in schedule_direct_cuda\n    ref_log = autotvm.tophub.load_reference_log(\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/autotvm/tophub.py\", line 220, in load_reference_log\n    download_package(tophub_location, package_name)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/autotvm/tophub.py\", line 182, in download_package\n    download(download_url, Path(rootpath, package_name), overwrite=True)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/contrib/download.py\", line 125, in download\n    raise err\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/site-packages/tvm/contrib/download.py\", line 112, in download\n    urllib2.urlretrieve(url, download_loc, reporthook=_download_progress)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/urllib/request.py\", line 241, in urlretrieve\n    with contextlib.closing(urlopen(url, data)) as fp:\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/urllib/request.py\", line 216, in urlopen\n    return opener.open(url, data, timeout)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/urllib/request.py\", line 519, in open\n    response = self._open(req, data)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/urllib/request.py\", line 536, in _open\n    result = self._call_chain(self.handle_open, protocol, protocol +\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n    result = func(*args)\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/urllib/request.py\", line 1391, in https_open\n    return self.do_open(http.client.HTTPSConnection, req,\n  File \"/home/xinet/anaconda3/envs/py310/lib/python3.10/urllib/request.py\", line 1351, in do_open\n    raise URLError(err)\nConnectionRefusedError: [Errno 111] Connection refused\nDuring handling of the above exception, another exception occurred:\n\nurllib.error.URLError: <urlopen error [Errno 111] Connection refused>"
          ]
        }
      ],
      "source": [
        "opt_level = 3\n",
        "target = tvm.target.cuda()\n",
        "with tvm.transform.PassContext(opt_level=opt_level):\n",
        "    lib = relay.build(mod, target, params=params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 运行生成库\n",
        "\n",
        "现在我们可以创建图执行器并在 Nvidia GPU 上运行该模块。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.00089283 0.00103331 0.0009094  0.00102275 0.00108751 0.00106737\n",
            " 0.00106262 0.00095838 0.00110792 0.00113151]\n"
          ]
        }
      ],
      "source": [
        "# create random input\n",
        "dev = tvm.cuda()\n",
        "data = np.random.uniform(-1, 1, size=data_shape).astype(\"float32\")\n",
        "# create module\n",
        "module = graph_executor.GraphModule(lib[\"default\"](dev))\n",
        "# set input and parameters\n",
        "module.set_input(\"data\", data)\n",
        "# run\n",
        "module.run()\n",
        "# get output\n",
        "out = module.get_output(0, tvm.nd.empty(out_shape)).numpy()\n",
        "\n",
        "# Print first 10 elements of output\n",
        "print(out.flatten()[0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 保存和加载已编译的模块\n",
        "\n",
        "也可以将 graph、lib 和参数保存到文件中，并在部署环境中加载它们。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['deploy_lib.tar']\n"
          ]
        }
      ],
      "source": [
        "# save the graph, lib and params into separate files\n",
        "from tvm.contrib import utils\n",
        "\n",
        "temp = utils.tempdir()\n",
        "path_lib = temp.relpath(\"deploy_lib.tar\")\n",
        "lib.export_library(path_lib)\n",
        "print(temp.listdir())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.00089283 0.00103331 0.0009094  0.00102275 0.00108751 0.00106737\n",
            " 0.00106262 0.00095838 0.00110792 0.00113151]\n"
          ]
        }
      ],
      "source": [
        "# load the module back.\n",
        "loaded_lib = tvm.runtime.load_module(path_lib)\n",
        "input_data = tvm.nd.array(data)\n",
        "\n",
        "module = graph_executor.GraphModule(loaded_lib[\"default\"](dev))\n",
        "module.run(data=input_data)\n",
        "out_deploy = module.get_output(0).numpy()\n",
        "\n",
        "# Print first 10 elements of output\n",
        "print(out_deploy.flatten()[0:10])\n",
        "\n",
        "# check whether the output from deployed module is consistent with original one\n",
        "tvm.testing.assert_allclose(out_deploy, out, atol=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "113215d72022e03bdca1057c7933c75a3969a189b0b522e71371adf999f86f27"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('py310': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
